{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df963bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏•‡∏á)\n",
    "# !pip install transformers scikit-learn pythainlp pandas gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833d196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: x-transformers in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (2.14.2)\n",
      "Requirement already satisfied: einops>=0.8.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from x-transformers) (0.8.1)\n",
      "Requirement already satisfied: einx>=0.3.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from x-transformers) (0.3.0)\n",
      "Requirement already satisfied: loguru in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from x-transformers) (0.7.3)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from x-transformers) (25.0)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from x-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from einx>=0.3.0->x-transformers) (1.26.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from einx>=0.3.0->x-transformers) (1.14.0)\n",
      "Requirement already satisfied: frozendict in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from einx>=0.3.0->x-transformers) (2.4.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from torch>=2.0->x-transformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from torch>=2.0->x-transformers) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from torch>=2.0->x-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from torch>=2.0->x-transformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from torch>=2.0->x-transformers) (2025.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from jinja2->torch>=2.0->x-transformers) (3.0.2)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from loguru->x-transformers) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from loguru->x-transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from sympy->einx>=0.3.0->x-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install x-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83106678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp import word_vector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from x_transformers import TransformerWrapper, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c34db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üü¢ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdab1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256        # ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÑ‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡πà‡∏≤‡∏ß\n",
    "BATCH_SIZE = 128     # Batch ‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Sequence ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á\n",
    "EPOCHS = 500         # ‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡πÅ‡∏ï‡πà‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡πÄ‡∏£‡∏≤‡∏°‡∏µ Early Stopping ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡∏î‡∏à‡∏ö\n",
    "PATIENCE = 20        # ‡∏ñ‡πâ‡∏≤ Loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á 20 ‡∏£‡∏≠‡∏ö‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f995a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_train.csv\")\n",
    "texts = df[\"body_text\"].astype(str).tolist()\n",
    "label_cols = [\n",
    "    \"politics\", \"human_rights\", \"quality_of_life\", \"international\",\n",
    "    \"social\", \"environment\", \"economics\", \"culture\", \"labor\",\n",
    "    \"national_security\", \"ict\", \"education\"\n",
    "]\n",
    "y = df[label_cols].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514d0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "tokenized_texts = [word_tokenize(t, keep_whitespace=False) for t in texts]\n",
    "thai2vec_vocab = list(w2v.key_to_index.keys())\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, word in enumerate(thai2vec_vocab, start=2):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f80ec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (48941, 256)\n",
      "Test shape: (5438, 256)\n"
     ]
    }
   ],
   "source": [
    "def encode_text(tokens, vocab):\n",
    "    return [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
    "\n",
    "encoded_texts = [encode_text(tokens, vocab) for tokens in tokenized_texts]\n",
    "\n",
    "def pad_sequences(sequences, max_len=None, pad_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = 256\n",
    "    \n",
    "    padded = np.full((len(sequences), max_len), pad_value, dtype=np.int64)\n",
    "    lengths = np.array([len(seq) for seq in sequences], dtype=np.int64)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = min(len(seq), max_len)\n",
    "        padded[i, :end] = seq[:end]\n",
    "        \n",
    "    return padded, lengths\n",
    "\n",
    "X, lengths = pad_sequences(encoded_texts, max_len=256)\n",
    "X_train, X_test, y_train, y_test, len_train, len_test = train_test_split(\n",
    "    X, y, lengths, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0866c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThaiTextDataset(Dataset):\n",
    "    def __init__(self, X, lengths, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.lengths[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ThaiTextDataset(X_train, len_train, y_train)\n",
    "test_dataset = ThaiTextDataset(X_test, len_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f2fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max(vocab.values()) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in w2v:\n",
    "        embedding_matrix[idx] = w2v[word]\n",
    "    elif word == \"<PAD>\":\n",
    "        embedding_matrix[idx] = np.zeros(embedding_dim)\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d386fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, depth, heads, output_dim, max_len, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ‡πÉ‡∏ä‡πâ Wrapper ‡∏Ç‡∏≠‡∏á x-transformers\n",
    "        self.model = TransformerWrapper(\n",
    "            num_tokens = vocab_size,\n",
    "            max_seq_len = max_len,\n",
    "            attn_layers = Encoder(\n",
    "                dim = embed_dim,\n",
    "                depth = depth,\n",
    "                heads = heads,\n",
    "                layer_dropout = 0.1,\n",
    "                attn_dropout = 0.1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Load Pretrained Thai2Fit weights\n",
    "        if embedding_matrix is not None:\n",
    "            weights = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "            try:\n",
    "                self.model.token_emb.emb.weight.data.copy_(weights)\n",
    "                self.model.token_emb.emb.weight.requires_grad = True\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    self.model.token_emb.weight.data.copy_(weights)\n",
    "                    self.model.token_emb.weight.requires_grad = True\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è Warning: Could not load pretrained embeddings.\")\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, lengths=None):\n",
    "        # üü¢ ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: return_embeddings=True \n",
    "        # ‡∏ö‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ß‡πà‡∏≤ \"‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≤‡∏¢‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ô‡∏∞\" (‡∏õ‡∏¥‡∏î Logits) -> ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÅ‡∏£‡∏°‡πÑ‡∏õ 90%\n",
    "        x = self.model(text, return_embeddings=True)\n",
    "        \n",
    "        # Mean Pooling\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1651fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: Embed=300, Heads=5, Depth=6\n"
     ]
    }
   ],
   "source": [
    "embed_dim = embedding_dim\n",
    "heads = 8 if embed_dim % 8 == 0 else (5 if embed_dim % 5 == 0 else 4)\n",
    "depth = 6 \n",
    "\n",
    "print(f\"Settings: Embed={embed_dim}, Heads={heads}, Depth={depth}\")\n",
    "\n",
    "model = XTransformerClassifier(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=embed_dim, \n",
    "    depth=depth, \n",
    "    heads=heads, \n",
    "    output_dim=len(label_cols), \n",
    "    max_len=MAX_LEN, \n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scaler = GradScaler() # üëà ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢ Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bb3253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Training (Batch=128)...\n",
      "Epoch 1/500 | Loss: 0.2592 | Time: 1096.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     loss = criterion(outputs, y_batch)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Backward ‡πÅ‡∏ö‡∏ö Scaled\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m scaler.unscale_(optimizer)\n\u001b[32m     26\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"üöÄ Starting Training (Batch={BATCH_SIZE})...\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, lengths_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Context\n",
    "        with autocast():\n",
    "            outputs = model(X_batch) \n",
    "            loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward ‡πÅ‡∏ö‡∏ö Scaled\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # --- Early Stopping ---\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_x_transformer_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"üõë Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"‚úÖ Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:179.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "F1-score (macro): 0.6319927595062403\n",
      "F1-score (micro): 0.6944149974892596\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_x_transformer_model.pth\"))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch, y_batch in test_loader:\n",
    "        X_batch, lengths_batch = X_batch.to(device), lengths_batch.to(device)\n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        y_true.append(y_batch.numpy())\n",
    "        y_pred.append(preds)\n",
    "\n",
    "y_true = np.vstack(y_true)\n",
    "y_pred = np.vstack(y_pred)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"F1-score (macro):\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"F1-score (micro):\", f1_score(y_true, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9959f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Prediction:\n",
      "1. [('quality_of_life', 0.971625804901123), ('international', 0.7255356311798096), ('environment', 0.6329044699668884), ('economics', 0.9968211650848389)]\n",
      "2. [('politics', 0.022722477093338966)]\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(text, keep_whitespace=False)\n",
    "    ids = encode_text(tokens, vocab)\n",
    "    \n",
    "    # ‡∏ï‡∏±‡∏î‡∏´‡∏£‡∏∑‡∏≠ Padding ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö MAX_LEN ‡πÄ‡∏™‡∏°‡∏≠\n",
    "    if len(ids) > MAX_LEN:\n",
    "        ids = ids[:MAX_LEN]\n",
    "    elif len(ids) < MAX_LEN:\n",
    "        ids = ids + [0] * (MAX_LEN - len(ids))\n",
    "\n",
    "    padded = torch.tensor([ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(padded) \n",
    "        probs = torch.sigmoid(output).cpu().numpy()[0]\n",
    "        \n",
    "    results = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        if prob > 0.5:\n",
    "            results.append((label_cols[i], float(prob)))\n",
    "            \n",
    "    if not results:\n",
    "        best_idx = np.argmax(probs)\n",
    "        results.append((label_cols[best_idx], float(probs[best_idx])))\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(predict(\"‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÑ‡∏ó‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
