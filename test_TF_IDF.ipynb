{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ca086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup & Stopwords Ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import display\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Path\n",
    "file_name = r\"d:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_test.csv\"\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î Stopwords\n",
    "stop_words = set(thai_stopwords())\n",
    "my_custom_stops = {\n",
    "    ' ', '\\n', '\\t', '‚Äú', '‚Äù', '(', ')', '[', ']', \n",
    "    '-', '.', ',', ':', '/', '?', '!',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
    "}\n",
    "stop_words.update(my_custom_stops)\n",
    "\n",
    "print(\"‚úÖ Setup & Stopwords Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fa7b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5 rows for testing.\n"
     ]
    }
   ],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "def text_process(text):\n",
    "    tokens = word_tokenize(str(text), engine='newmm')\n",
    "    clean_tokens = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words and w.strip() != '':\n",
    "            clean_tokens.append(w)\n",
    "    \n",
    "    # ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á \n",
    "    # (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ TfidfVectorizer ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ input ‡πÄ‡∏õ‡πá‡∏ô string ‡∏¢‡∏≤‡∏ß‡πÜ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ)\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "if os.path.exists(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 5 ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "    df_small = df.head(5).copy()\n",
    "    print(f\"‚úÖ Loaded {len(df_small)} rows for testing.\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: File not found at {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aaec7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Tokenizing & Cleaning)...\n",
      "‚úÖ Done!\n",
      "\n",
      "--- üîç ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17 ‡∏û.‡∏¢. 2558 Blognone [1] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å...</td>\n",
       "      <td>17 ‡∏û ‡∏¢ 2558 Blognone [1] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà ‡∏™.‡∏™. ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ...</td>\n",
       "      <td>‡∏™.‡∏™. ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÅ‡∏Å‡πâ ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç ‡∏Å‡∏≠‡∏á‡∏Å‡∏≥‡∏•‡∏±‡∏á ‡∏õ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text  \\\n",
       "0  17 ‡∏û.‡∏¢. 2558 Blognone [1] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å...   \n",
       "1  ‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà ‡∏™.‡∏™. ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  17 ‡∏û ‡∏¢ 2558 Blognone [1] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anony...  \n",
       "1  ‡∏™.‡∏™. ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÅ‡∏Å‡πâ ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç ‡∏Å‡∏≠‡∏á‡∏Å‡∏≥‡∏•‡∏±‡∏á ‡∏õ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Tokenizing & Cleaning)...\")\n",
    "df_small['clean_text'] = df_small['body_text'].apply(text_process)\n",
    "\n",
    "print(\"‚úÖ Done!\")\n",
    "print(\"\\n--- üîç ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\")\n",
    "display(df_small[['body_text', 'clean_text']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e43a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Feature Extraction ---\n",
      "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô): (5, 735)\n"
     ]
    }
   ],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏•‡∏á TF-IDF\n",
    "# tokenizer=lambda x: x.split() -> ‡∏ö‡∏≠‡∏Å‡∏°‡∏±‡∏ô‡∏ß‡πà‡∏≤ \"‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞ ‡∏â‡∏±‡∏ô‡∏ï‡∏±‡∏î‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏Ñ‡πà‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏≠\"\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), token_pattern=None)\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_small['clean_text'])\n",
    "\n",
    "print(\"--- üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Feature Extraction ---\")\n",
    "print(f\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô): {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16171b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ 15 ‡∏Ñ‡∏≥‡πÄ‡∏î‡πà‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1 (Top Keywords) ---\n",
      "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß: ‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anonymous ‡∏•‡∏±‡πà‡∏ô‡∏ó‡∏≥‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏±‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏° IS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anonymous</th>\n",
       "      <td>0.514394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.367424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡πÇ‡∏à‡∏°‡∏ï‡∏µ</th>\n",
       "      <td>0.296435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå</th>\n",
       "      <td>0.220454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°</th>\n",
       "      <td>0.177861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏Å‡∏£‡∏∏‡∏á</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏ö‡∏±‡∏ç‡∏ä‡∏µ</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏õ‡∏≤‡∏£‡∏µ‡∏™</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charlie</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏≠‡∏≠‡∏Å‡∏°‡∏≤</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hebdo</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blognone</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå</th>\n",
       "      <td>0.146970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf_score\n",
       "anonymous        0.514394\n",
       "is               0.367424\n",
       "‡πÇ‡∏à‡∏°‡∏ï‡∏µ            0.296435\n",
       "‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå         0.220454\n",
       "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°     0.177861\n",
       "‡∏Å‡∏£‡∏∏‡∏á             0.146970\n",
       "‡∏ö‡∏±‡∏ç‡∏ä‡∏µ            0.146970\n",
       "‡∏õ‡∏≤‡∏£‡∏µ‡∏™            0.146970\n",
       "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå       0.146970\n",
       "charlie          0.146970\n",
       "‡∏≠‡∏≠‡∏Å‡∏°‡∏≤            0.146970\n",
       "hebdo            0.146970\n",
       "blognone         0.146970\n",
       "‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™         0.146970\n",
       "‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå          0.146970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===========================\n",
    "# üîç ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡∏π‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà (0-4)\n",
    "doc_idx = 0 \n",
    "# ===========================\n",
    "\n",
    "if doc_idx < len(df_small):\n",
    "    # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤\n",
    "    first_document_vector = tfidf_matrix[doc_idx]\n",
    "\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataframe ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏ß‡∏¢‡πÜ\n",
    "    df_tfidf = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf_score\"])\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢\n",
    "    top_keywords = df_tfidf.sort_values(by=[\"tfidf_score\"], ascending=False).head(15)\n",
    "\n",
    "    print(f\"--- üèÜ 15 ‡∏Ñ‡∏≥‡πÄ‡∏î‡πà‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {doc_idx+1} (Top Keywords) ---\")\n",
    "    print(f\"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß: {df_small['title'].iloc[doc_idx]}\")\n",
    "    display(top_keywords)\n",
    "else:\n",
    "    print(\"‚ùå ‡πÄ‡∏•‡∏Ç Index ‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
