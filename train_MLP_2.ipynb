{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb035284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.util import normalize\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "# ==========================================\n",
    "# 0. Configuration & Seeding\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random Seed set to {seed}\")\n",
    "\n",
    "# set_seed(42) \n",
    "\n",
    "CSV_PATH = r\"d:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_train.csv\" # ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Å‡πâ path ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô\n",
    "W2V_PATH = \"custom_word2vec.model\"\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005 #‡∏¢‡∏¥‡πà‡∏á‡∏•‡∏î‡∏¢‡∏¥‡πà‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "THRESHOLD = 0.5 \n",
    "HIDDEN_DIM1 = 256\n",
    "HIDDEN_DIM2 = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69af9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data & Resources ---\n",
      "-> Loaded custom_word2vec.model successfully.\n",
      "Number of Classes: 12\n",
      "Converting text to vectors...\n",
      "Data Preparation Complete!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Loading Data & Resources\n",
    "# ==========================================\n",
    "print(\"--- Step 1: Loading Data & Resources ---\")\n",
    "try:\n",
    "    w2v_model = Word2Vec.load(W2V_PATH) \n",
    "    print(f\"-> Loaded {W2V_PATH} successfully.\")\n",
    "except:\n",
    "    print(f\"Error: Could not load {W2V_PATH}\")\n",
    "    # ‡πÉ‡∏ô Jupyter ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ exit() ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡∏õ‡∏¥‡∏î Kernel ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ raise Error ‡πÅ‡∏ó‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà print\n",
    "    raise FileNotFoundError(f\"Model not found: {W2V_PATH}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "# Prepare Labels\n",
    "label_cols = ['politics', 'human_rights', 'quality_of_life', 'international', \n",
    "              'social', 'environment', 'economics', 'culture', 'labor', \n",
    "              'national_security', 'ict', 'education']\n",
    "y_numpy = df[label_cols].values \n",
    "num_classes = len(label_cols)\n",
    "print(f\"Number of Classes: {num_classes}\")\n",
    "\n",
    "# Prepare Features\n",
    "stop_words = set(thai_stopwords())\n",
    "\n",
    "def get_avg_vector(text):\n",
    "    text = normalize(str(text))\n",
    "    tokens = word_tokenize(str(text), engine='newmm')\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and word.strip() != '':\n",
    "            if word in w2v_model.wv.key_to_index:\n",
    "                vecs.append(w2v_model.wv[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(300) \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "print(\"Converting text to vectors...\")\n",
    "X_numpy = np.vstack(df['body_text'].apply(get_avg_vector).values)\n",
    "\n",
    "# Split & DataLoader\n",
    "X_tensor = torch.tensor(X_numpy, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_numpy, dtype=torch.float32).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Data Preparation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35400154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Architecture ---\n",
      "MultiLabelMLP(\n",
      "  (layer1): Linear(in_features=300, out_features=256, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (layer3): Linear(in_features=128, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. Model Definition\n",
    "# ==========================================\n",
    "class MultiLabelMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(MultiLabelMLP, self).__init__()\n",
    "        # Layer 1\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5) \n",
    "        # Layer 2\n",
    "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Layer 3 (Output)\n",
    "        self.layer3 = nn.Linear(hidden_dim2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "model = MultiLabelMLP(300, HIDDEN_DIM1, HIDDEN_DIM2, num_classes).to(device)\n",
    "print(\"\\n--- Model Architecture ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db178846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Training (1000 Epochs) ---\n",
      "Epoch [100/1000], Loss: 0.1466 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [200/1000], Loss: 0.1439 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [300/1000], Loss: 0.1424 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [400/1000], Loss: 0.1410 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [500/1000], Loss: 0.1383 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [600/1000], Loss: 0.1381 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [700/1000], Loss: 0.1367 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [800/1000], Loss: 0.1356 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [900/1000], Loss: 0.1353 | Best F1: 70.84% (Ep 70)\n",
      "Epoch [1000/1000], Loss: 0.1343 | Best F1: 70.84% (Ep 70)\n",
      "\n",
      "Training Complete. Loading best model from Epoch 70 (F1: 70.84%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. Training Process\n",
    "# ==========================================\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\n--- Step 2: Training ({NUM_EPOCHS} Epochs) ---\")\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation & Save Best\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            probs = torch.sigmoid(test_outputs)\n",
    "            predicted = (probs > THRESHOLD).float()\n",
    "            current_f1 = f1_score(y_test.cpu().numpy(), predicted.cpu().numpy(), average='micro')\n",
    "            \n",
    "            if current_f1 > best_f1:\n",
    "                best_f1 = current_f1\n",
    "                best_epoch = epoch + 1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_mlp_model.pth')\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {total_loss/len(train_loader):.4f} | Best F1: {best_f1*100:.2f}% (Ep {best_epoch})\")\n",
    "\n",
    "# Load Best Model\n",
    "print(f\"\\nTraining Complete. Loading best model from Epoch {best_epoch} (F1: {best_f1*100:.2f}%)\")\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6bb456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Evaluation (Best Model) ---\n",
      "F1 Score (Micro): 70.84%\n",
      "F1 Score (Samples): 69.53%\n",
      "\n",
      "--- Classification Report ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         politics       0.80      0.86      0.83      6369\n",
      "     human_rights       0.74      0.61      0.67      2408\n",
      "  quality_of_life       0.71      0.58      0.64      1819\n",
      "    international       0.81      0.73      0.77      1241\n",
      "           social       0.68      0.12      0.21      1276\n",
      "      environment       0.72      0.65      0.68      1213\n",
      "        economics       0.69      0.57      0.62       831\n",
      "          culture       0.66      0.48      0.55       626\n",
      "            labor       0.80      0.80      0.80       567\n",
      "national_security       0.67      0.37      0.48       546\n",
      "              ict       0.71      0.70      0.71       492\n",
      "        education       0.70      0.40      0.51       411\n",
      "\n",
      "        micro avg       0.76      0.66      0.71     17799\n",
      "        macro avg       0.72      0.57      0.62     17799\n",
      "     weighted avg       0.75      0.66      0.69     17799\n",
      "      samples avg       0.75      0.69      0.70     17799\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. Evaluation\n",
    "# ==========================================\n",
    "print(\"\\n--- Step 3: Evaluation (Best Model) ---\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    probs = torch.sigmoid(test_outputs)\n",
    "    predicted = (probs > THRESHOLD).float()\n",
    "    \n",
    "    y_true = y_test.cpu().numpy()\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    \n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_samples = f1_score(y_true, y_pred, average='samples')\n",
    "    \n",
    "    print(f\"F1 Score (Micro): {f1_micro*100:.2f}%\")\n",
    "    print(f\"F1 Score (Samples): {f1_samples*100:.2f}%\")\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e01cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Functions for Testing\n",
    "# ==========================================\n",
    "def predict_custom_news(text):\n",
    "    model.eval()\n",
    "    vec = get_avg_vector(text)\n",
    "    tensor = torch.tensor(vec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n--- Testing Custom News ---\")\n",
    "    print(f\"Snippet: {text[:50]}...\")\n",
    "    found_labels = []\n",
    "    for i, col in enumerate(label_cols):\n",
    "        if probs[i] > THRESHOLD:\n",
    "            print(f\"[/] {col}: {probs[i]*100:.2f}% (YES)\")\n",
    "            found_labels.append(col)\n",
    "        elif probs[i] > 0.1: \n",
    "            print(f\"[ ] {col}: {probs[i]*100:.2f}%\")\n",
    "    print(f\">> Result: {found_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f831adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to stop.\n",
      "\n",
      "--- Testing Custom News ---\n",
      "Snippet: ‡∏ß‡∏á‡πÄ‡∏™‡∏ß‡∏ô‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏°‡∏∏‡∏ó‡∏¥‡∏ï‡∏≤‡∏à‡∏¥‡∏ï ‚Äò‡∏ô‡∏•‡∏¥‡∏ô‡∏µ ‡∏ï‡∏±‡∏ô‡∏ò‡∏∏‡∏ß‡∏ô‡∏¥‡∏ï‡∏¢‡πå‚Äô ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‚Äú...\n",
      "[ ] politics: 15.76%\n",
      "[ ] human_rights: 22.68%\n",
      "[ ] quality_of_life: 39.46%\n",
      "[/] social: 51.13% (YES)\n",
      "[/] culture: 66.69% (YES)\n",
      "[ ] education: 38.73%\n",
      ">> Result: ['social', 'culture']\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. Interactive Testing\n",
    "# ==========================================\n",
    "print(\"Type 'exit' to stop.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\\nüìù Enter news text: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        if not user_input: continue\n",
    "            \n",
    "        predict_custom_news(user_input)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
