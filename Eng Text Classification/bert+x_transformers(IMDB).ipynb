{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Ri3igK0Bxs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from x_transformers import Decoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BERT_NAME = \"bert-base-uncased\"\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 4\n",
        "LR = 2e-4\n",
        "NUM_CLASSES = 2"
      ],
      "metadata": {
        "id": "mE07fRUE07pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_imdb(path):\n",
        "    df = pd.read_csv(path)\n",
        "    texts = df[\"review\"].tolist()\n",
        "    labels = [1 if s == \"positive\" else 0 for s in df[\"sentiment\"]]\n",
        "    return texts, labels\n",
        "\n",
        "data_path = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
        "texts, labels = load_imdb(data_path)"
      ],
      "metadata": {
        "id": "VC-zYyTN09Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(BERT_NAME)\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "liErUFqG09Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertXTransformerClassifier(nn.Module):\n",
        "    def __init__(self, freeze_bert=True):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(BERT_NAME)\n",
        "\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.hidden_dim = self.bert.config.hidden_size\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            dim=self.hidden_dim,\n",
        "            depth=4,\n",
        "            heads=8\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(self.hidden_dim, NUM_CLASSES)"
      ],
      "metadata": {
        "id": "-qKB_Xy809T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def encode_with_bert(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return outputs.last_hidden_stat"
      ],
      "metadata": {
        "id": "HaHHpPuh09Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def decode_with_xtransformer(self, hidden_states):\n",
        "        return self.decoder(hidden_states)"
      ],
      "metadata": {
        "id": "zbV_iMzk09OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input_ids, attention_mask):\n",
        "        encoded = self.encode_with_bert(input_ids, attention_mask)\n",
        "        decoded = self.decode_with_xtransformer(encoded)\n",
        "        pooled = decoded.mean(dim=1)\n",
        "        return self.classifier(pooled)"
      ],
      "metadata": {
        "id": "eSMj4I-K09LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(\n",
        "            batch[\"input_ids\"].to(DEVICE),\n",
        "            batch[\"attention_mask\"].to(DEVICE)\n",
        "        )\n",
        "        loss = loss_fn(logits, batch[\"labels\"].to(DEVICE))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "SU_1Qx1u1bOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            logits = model(\n",
        "                batch[\"input_ids\"].to(DEVICE),\n",
        "                batch[\"attention_mask\"].to(DEVICE)\n",
        "            )\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            y_true.extend(batch[\"labels\"].numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
        "\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    return acc, f1"
      ],
      "metadata": {
        "id": "wRGdjQw51c5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    IMDBDataset(train_texts, train_labels),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    IMDBDataset(val_texts, val_labels),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "model = BertXTransformerClassifier(freeze_bert=True).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=LR\n",
        ")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "    train_epoch(model, train_loader, optimizer)\n",
        "    evaluate(model, val_loader)"
      ],
      "metadata": {
        "id": "LWV1LMor1c2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bert_xtransformer_split.pth\")"
      ],
      "metadata": {
        "id": "F6poj68A1c0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    model.eval()\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            enc[\"input_ids\"].to(DEVICE),\n",
        "            enc[\"attention_mask\"].to(DEVICE)\n",
        "        )\n",
        "        return \"positive\" if torch.argmax(logits, dim=1).item() == 1 else \"negative\"\n",
        "\n",
        "print(\"\\nTest prediction:\")\n",
        "print(predict(\"This movie was absolutely fantastic and emotional.\"))"
      ],
      "metadata": {
        "id": "rxzikoGr1hhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}