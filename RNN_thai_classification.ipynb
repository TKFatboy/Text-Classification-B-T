{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkwJt_VLWJpR",
        "outputId": "18b7600c-4be0-42a5-dcf0-bc526e157824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (4.57.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: pythainlp in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (5.1.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: gensim in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (4.4.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tzdata in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from pythainlp) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from requests->transformers) (2.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: wrapt in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nawapol\\anaconda3\\envs\\pt\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers scikit-learn pythainlp pandas gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Zx3LLdHsWD0c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pythainlp import word_tokenize\n",
        "from pythainlp import word_vector\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cGzM6LazfEUe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üü¢ Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üü¢ Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YpPglN4eWHII"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\y'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\y'\n",
            "C:\\Users\\nawapol\\AppData\\Local\\Temp\\ipykernel_1700\\1576565176.py:1: SyntaxWarning: invalid escape sequence '\\y'\n",
            "  df = pd.read_csv(\"D:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_train.csv\")\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"D:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_train.csv\")\n",
        "texts = df[\"body_text\"].astype(str).tolist()\n",
        "label_cols = [\n",
        "    \"politics\", \"human_rights\", \"quality_of_life\", \"international\",\n",
        "    \"social\", \"environment\", \"economics\", \"culture\", \"labor\",\n",
        "    \"national_security\", \"ict\", \"education\"\n",
        "]\n",
        "y = df[label_cols].values.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "e-esIZxlWq_Q"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m w2v = word_vector.WordVector(model_name=\u001b[33m\"\u001b[39m\u001b[33mthai2fit_wv\u001b[39m\u001b[33m\"\u001b[39m).get_model()\n\u001b[32m      2\u001b[39m embedding_dim = w2v.vector_size\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenized_texts = [\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_whitespace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m      5\u001b[39m thai2vec_vocab = \u001b[38;5;28mlist\u001b[39m(w2v.key_to_index.keys())\n\u001b[32m      6\u001b[39m vocab = {\u001b[33m\"\u001b[39m\u001b[33m<PAD>\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m<UNK>\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\pythainlp\\tokenize\\core.py:239\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, custom_dict, engine, keep_whitespace, join_broken_num)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mnewmm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33monecut\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpythainlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnewmm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m segment\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     segments = \u001b[43msegment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mnewmm-safe\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpythainlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnewmm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m segment\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\pythainlp\\tokenize\\newmm.py:171\u001b[39m, in \u001b[36msegment\u001b[39m\u001b[34m(text, custom_dict, safe_mode)\u001b[39m\n\u001b[32m    168\u001b[39m     custom_dict = DEFAULT_WORD_DICT_TRIE\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m safe_mode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) < _TEXT_SCAN_END:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_onecut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# if the text is longer than the limit,\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# break them into smaller chunks, then tokenize each chunk\u001b[39;00m\n\u001b[32m    175\u001b[39m text_parts = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\pythainlp\\tokenize\\newmm.py:85\u001b[39m, in \u001b[36m_onecut\u001b[39m\u001b[34m(text, custom_dict)\u001b[39m\n\u001b[32m     83\u001b[39m pos_list = [\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# priority queue of possible breaking positions\u001b[39;00m\n\u001b[32m     84\u001b[39m end_pos = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m pos_list[\u001b[32m0\u001b[39m] < len_text:\n\u001b[32m     86\u001b[39m     begin_pos = heappop(pos_list)\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m custom_dict.prefixes(text[begin_pos:]):\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "w2v = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model()\n",
        "embedding_dim = w2v.vector_size\n",
        "\n",
        "tokenized_texts = [word_tokenize(t, keep_whitespace=False) for t in texts]\n",
        "thai2vec_vocab = list(w2v.key_to_index.keys())\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "for i, word in enumerate(thai2vec_vocab, start=2):\n",
        "    vocab[word] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUIE1Qx8XHPh"
      },
      "outputs": [],
      "source": [
        "def encode_text(tokens, vocab):\n",
        "    return [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
        "\n",
        "encoded_texts = [encode_text(tokens, vocab) for tokens in tokenized_texts]\n",
        "\n",
        "def pad_sequences(sequences, max_len=None, pad_value=0):\n",
        "    if not max_len:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded = np.full((len(sequences), max_len), pad_value, dtype=np.int64)\n",
        "    lengths = np.array([len(seq) for seq in sequences], dtype=np.int64)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        padded[i, :len(seq)] = seq[:max_len]\n",
        "    return padded, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJJbqlt0XMx7"
      },
      "outputs": [],
      "source": [
        "X, lengths = pad_sequences(encoded_texts)\n",
        "X_train, X_test, y_train, y_test, len_train, len_test = train_test_split(\n",
        "    X, y, lengths, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Yf79OfXQaP"
      },
      "outputs": [],
      "source": [
        "class ThaiTextDataset(Dataset):\n",
        "    def __init__(self, X, lengths, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.lengths[idx], self.y[idx]\n",
        "\n",
        "train_dataset = ThaiTextDataset(X_train, len_train, y_train)\n",
        "test_dataset = ThaiTextDataset(X_test, len_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI4EXECge9TI"
      },
      "outputs": [],
      "source": [
        "vocab_size = max(vocab.values()) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in vocab.items():\n",
        "    if word in w2v:\n",
        "        embedding_matrix[idx] = w2v[word]\n",
        "    elif word == \"<PAD>\":\n",
        "        embedding_matrix[idx] = np.zeros(embedding_dim)\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DXmO1_rXYig"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size, output_dim, embedding_matrix=None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), enforce_sorted=False, batch_first=True)\n",
        "        packed_out, (hidden, cell) = self.rnn(packed)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "embed_dim = embedding_dim\n",
        "rnn_hidden_size = 128\n",
        "fc_hidden_size = 64\n",
        "output_dim = len(label_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJEgHljOXtLo"
      },
      "outputs": [],
      "source": [
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size, output_dim, embedding_matrix).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, lengths_batch, y_batch in train_loader:\n",
        "        X_batch, lengths_batch, y_batch = X_batch.to(device), lengths_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch, lengths_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4S1J91BX1AH"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, lengths_batch, y_batch in test_loader:\n",
        "        X_batch, lengths_batch = X_batch.to(device), lengths_batch.to(device)\n",
        "        outputs = model(X_batch, lengths_batch)\n",
        "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "        preds = (preds > 0.5).astype(int)\n",
        "        y_true.append(y_batch.numpy())\n",
        "        y_pred.append(preds)\n",
        "\n",
        "y_true = np.vstack(y_true)\n",
        "y_pred = np.vstack(y_pred)\n",
        "\n",
        "print(\"F1-score (macro):\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "print(\"F1-score (micro):\", f1_score(y_true, y_pred, average=\"micro\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClpYIaiAYU1k"
      },
      "outputs": [],
      "source": [
        "def predict(text):\n",
        "    model.eval()\n",
        "    tokens = word_tokenize(text, keep_whitespace=False)\n",
        "    ids = encode_text(tokens, vocab)\n",
        "    lengths = torch.tensor([len(ids)], dtype=torch.long).to(device)\n",
        "    padded = torch.tensor([ids], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(padded, lengths)\n",
        "        probs = torch.sigmoid(output).cpu().numpy()[0]\n",
        "        best_idx = np.argmax(probs)\n",
        "        return label_cols[best_idx], float(probs[best_idx])\n",
        "\n",
        "print(predict(\"‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÑ‡∏ó‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà\"))\n",
        "print(predict(\"‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏ó‡πâ‡∏ß‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
