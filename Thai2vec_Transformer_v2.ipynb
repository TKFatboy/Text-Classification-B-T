{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import math\n",
                "import time\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "from pythainlp import word_tokenize, word_vector\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import f1_score, classification_report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =====================================================\n",
                "# HYPERPARAMETERS (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î)\n",
                "# =====================================================\n",
                "MAX_LEN = 256\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 500\n",
                "LEARNING_RATE = 2e-4  # Fixed LR - ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏≠‡∏á\n",
                "EMBED_DIM = 300\n",
                "DEPTH = 4\n",
                "HEADS = 4\n",
                "THRESHOLD = 0.5\n",
                "PATIENCE = 25  # Early stopping patience\n",
                "\n",
                "# Train/Test/Valid Split Ratios\n",
                "TRAIN_RATIO = 0.8   # 80%\n",
                "TEST_RATIO = 0.1    # 10%\n",
                "VALID_RATIO = 0.1   # 10%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üü¢ Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# DEVICE SETUP\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"üü¢ Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LOAD DATA\n",
                "df = pd.read_csv(r\"D:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_train.csv\")\n",
                "texts = df[\"body_text\"].astype(str).tolist()\n",
                "label_cols = [\n",
                "    \"politics\", \"human_rights\", \"quality_of_life\", \"international\",\n",
                "    \"social\", \"environment\", \"economics\", \"culture\", \"labor\",\n",
                "    \"national_security\", \"ict\", \"education\"\n",
                "]\n",
                "y = df[label_cols].values.astype(np.float32)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì• Loading Thai2Fit Word Vectors...\n",
                        "‚úÖ Loaded! Embedding dim: 300\n"
                    ]
                }
            ],
            "source": [
                "# LOAD THAI2FIT WORD VECTORS\n",
                "print(\"üì• Loading Thai2Fit Word Vectors...\")\n",
                "w2v = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model()\n",
                "embedding_dim = w2v.vector_size  # Should be 300\n",
                "vocab_list = list(w2v.key_to_index.keys())\n",
                "print(f\"‚úÖ Loaded! Embedding dim: {embedding_dim}\")\n",
                "\n",
                "# Build vocab\n",
                "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
                "for i, word in enumerate(vocab_list, start=2):\n",
                "    vocab[word] = i"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üî§ Tokenizing texts...\n"
                    ]
                }
            ],
            "source": [
                "# TOKENIZE & ENCODE\n",
                "print(\"üî§ Tokenizing texts...\")\n",
                "tokenized_texts = [word_tokenize(t, keep_whitespace=False) for t in texts]\n",
                "\n",
                "def encode_text(tokens, vocab):\n",
                "    return [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
                "\n",
                "encoded_texts = [encode_text(tokens, vocab) for tokens in tokenized_texts]\n",
                "print(\"‚úÖ Tokenization complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PAD SEQUENCES\n",
                "def pad_sequences(sequences, max_len=256, pad_value=0):\n",
                "    padded = np.full((len(sequences), max_len), pad_value, dtype=np.int64)\n",
                "    lengths = np.array([min(len(seq), max_len) for seq in sequences], dtype=np.int64)\n",
                "    for i, seq in enumerate(sequences):\n",
                "        end = min(len(seq), max_len)\n",
                "        padded[i, :end] = seq[:end]\n",
                "    return padded, lengths\n",
                "\n",
                "X, lengths = pad_sequences(encoded_texts, max_len=MAX_LEN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SPLIT DATA: 80% Train, 10% Test, 10% Valid\n",
                "print(\"üìä Splitting data: 80% train, 10% test, 10% valid...\")\n",
                "\n",
                "# First split: 80% train, 20% temp\n",
                "X_train, X_temp, y_train, y_temp, len_train, len_temp = train_test_split(\n",
                "    X, y, lengths, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Second split: 50% test, 50% valid from temp (= 10% each of total)\n",
                "X_test, X_val, y_test, y_val, len_test, len_val = train_test_split(\n",
                "    X_temp, y_temp, len_temp, test_size=0.5, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"  Train: {len(X_train)} samples\")\n",
                "print(f\"  Test:  {len(X_test)} samples\")\n",
                "print(f\"  Valid: {len(X_val)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DATASET & DATALOADER\n",
                "class ThaiTextDataset(Dataset):\n",
                "    def __init__(self, X, lengths, y):\n",
                "        self.X = torch.tensor(X, dtype=torch.long)\n",
                "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
                "        self.y = torch.tensor(y, dtype=torch.float32)\n",
                "    def __len__(self): return len(self.y)\n",
                "    def __getitem__(self, idx): return self.X[idx], self.lengths[idx], self.y[idx]\n",
                "\n",
                "train_loader = DataLoader(ThaiTextDataset(X_train, len_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_loader = DataLoader(ThaiTextDataset(X_test, len_test, y_test), batch_size=BATCH_SIZE)\n",
                "val_loader = DataLoader(ThaiTextDataset(X_val, len_val, y_val), batch_size=BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# BUILD EMBEDDING MATRIX\n",
                "vocab_size = len(vocab)\n",
                "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
                "for word, idx in vocab.items():\n",
                "    if word in w2v:\n",
                "        embedding_matrix[idx] = w2v[word]\n",
                "    else:\n",
                "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TRANSFORMER MODEL\n",
                "class TransformerClassifier(nn.Module):\n",
                "    def __init__(self, vocab_size, embed_dim, max_seq_len, output_dim, depth=4, heads=4, embedding_matrix=None):\n",
                "        super(TransformerClassifier, self).__init__()\n",
                "        self.embed_dim = embed_dim\n",
                "        \n",
                "        # 1. Embedding\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "        if embedding_matrix is not None:\n",
                "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
                "            self.embedding.weight.requires_grad = True  # Fine-tune embedding\n",
                "            \n",
                "        # 2. Positional Encoding\n",
                "        self.pos_encoder = nn.Parameter(torch.randn(1, max_seq_len, embed_dim))\n",
                "        \n",
                "        # 3. Transformer Encoder\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=embed_dim, \n",
                "            nhead=heads, \n",
                "            dim_feedforward=embed_dim * 4,\n",
                "            dropout=0.1,\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
                "        \n",
                "        # 4. Output\n",
                "        self.fc = nn.Linear(embed_dim, output_dim)\n",
                "        self.dropout = nn.Dropout(0.1)\n",
                "\n",
                "    def forward(self, x, lengths):\n",
                "        # Create Padding Mask\n",
                "        src_key_padding_mask = (x == 0)\n",
                "\n",
                "        # Embed + Pos Encode\n",
                "        x = self.embedding(x) * math.sqrt(self.embed_dim)\n",
                "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
                "        \n",
                "        # Transformer Pass\n",
                "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
                "        \n",
                "        # Mean Pooling\n",
                "        x = x.mean(dim=1) \n",
                "        \n",
                "        x = self.dropout(x)\n",
                "        return self.fc(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CREATE MODEL\n",
                "model = TransformerClassifier(\n",
                "    vocab_size=vocab_size,\n",
                "    embed_dim=embedding_dim,  # 300\n",
                "    max_seq_len=MAX_LEN,      # 256\n",
                "    output_dim=len(label_cols),\n",
                "    depth=DEPTH,              # 4\n",
                "    heads=HEADS,              # 4\n",
                "    embedding_matrix=embedding_matrix\n",
                ").to(device)\n",
                "\n",
                "print(f\"\\nüì¶ Model created with:\")\n",
                "print(f\"  - embed_dim: {embedding_dim}\")\n",
                "print(f\"  - depth: {DEPTH}\")\n",
                "print(f\"  - heads: {HEADS}\")\n",
                "print(f\"  - max_seq_len: {MAX_LEN}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TRAINING (Fixed LR - No Scheduler)\n",
                "criterion = nn.BCEWithLogitsLoss()\n",
                "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
                "scaler = GradScaler()\n",
                "\n",
                "# ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ scheduler - LR ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà 2e-4\n",
                "\n",
                "print(f\"üöÄ Start Training... (Max Epochs: {EPOCHS})\")\n",
                "print(f\"  - Learning Rate: {LEARNING_RATE} (Fixed)\")\n",
                "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"  - Patience: {PATIENCE}\")\n",
                "\n",
                "best_val_loss = float('inf')\n",
                "patience_counter = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    start_time = time.time()\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for X_batch, lengths_batch, y_batch in train_loader:\n",
                "        X_batch, lengths_batch, y_batch = X_batch.to(device), lengths_batch.to(device), y_batch.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with autocast():  # Mixed Precision\n",
                "            outputs = model(X_batch, lengths_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    avg_train_loss = total_loss / len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for X_batch, lengths_batch, y_batch in val_loader:\n",
                "            X_batch, lengths_batch, y_batch = X_batch.to(device), lengths_batch.to(device), y_batch.to(device)\n",
                "            outputs = model(X_batch, lengths_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            val_loss += loss.item()\n",
                "    \n",
                "    avg_val_loss = val_loss / len(val_loader)\n",
                "    val_time = time.time() - start_time\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {LEARNING_RATE} | Time: {val_time:.1f}s\")\n",
                "    \n",
                "    # Early Stopping based on Validation Loss\n",
                "    if avg_val_loss < best_val_loss:\n",
                "        best_val_loss = avg_val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save(model.state_dict(), 'best_model_thai2vec_transformer_v2.pth')\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= PATIENCE:\n",
                "            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n",
                "            break\n",
                "\n",
                "print(\"‚úÖ Training Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EVALUATION ON TEST SET\n",
                "model.load_state_dict(torch.load('best_model_thai2vec_transformer_v2.pth'))\n",
                "model.eval()\n",
                "\n",
                "y_true, y_pred = [], []\n",
                "with torch.no_grad():\n",
                "    for X_batch, lengths_batch, y_batch in test_loader:\n",
                "        X_batch, lengths_batch = X_batch.to(device), lengths_batch.to(device)\n",
                "        outputs = model(X_batch, lengths_batch)\n",
                "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
                "        preds = (preds > THRESHOLD).astype(int)\n",
                "        y_true.append(y_batch.numpy())\n",
                "        y_pred.append(preds)\n",
                "\n",
                "y_true = np.vstack(y_true)\n",
                "y_pred = np.vstack(y_pred)\n",
                "\n",
                "print(\"\\n\" + \"=\"*35)\n",
                "print(\"üèÜ TEST SET RESULTS\")\n",
                "print(\"=\"*35)\n",
                "print(f\"F1-Macro: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
                "print(f\"F1-Micro: {f1_score(y_true, y_pred, average='micro'):.4f}\")\n",
                "print(\"-\" * 35)\n",
                "print(classification_report(y_true, y_pred, target_names=label_cols, zero_division=0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PREDICT FUNCTION\n",
                "def predict(text, show_all=True, threshold=THRESHOLD):\n",
                "    \"\"\"\n",
                "    show_all: ‡∏ñ‡πâ‡∏≤ True ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)\n",
                "              ‡∏ñ‡πâ‡∏≤ False ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô threshold\n",
                "    threshold: ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    tokens = word_tokenize(text, keep_whitespace=False)\n",
                "    ids = [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
                "    \n",
                "    if len(ids) > MAX_LEN: ids = ids[:MAX_LEN]\n",
                "    else: ids = ids + [0] * (MAX_LEN - len(ids))\n",
                "    \n",
                "    tensor_in = torch.tensor([ids], dtype=torch.long).to(device)\n",
                "    len_in = torch.tensor([min(len(tokens), MAX_LEN)], dtype=torch.long).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        output = model(tensor_in, len_in)\n",
                "        probs = torch.sigmoid(output).cpu().numpy()[0]\n",
                "    \n",
                "    all_results = [(label_cols[i], float(prob)) for i, prob in enumerate(probs)]\n",
                "    all_results.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    if show_all:\n",
                "        return all_results\n",
                "    else:\n",
                "        filtered = [(name, prob) for name, prob in all_results if prob > threshold]\n",
                "        if not filtered:\n",
                "            filtered = [all_results[0]]\n",
                "        return filtered"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test prediction\n",
                "print(\"üîÆ Sample Prediction:\")\n",
                "sample = \"‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô\"\n",
                "print(f\"Input: {sample}\")\n",
                "print(f\"Output: {predict(sample, show_all=False)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "PT",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
