{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0beeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed set to 42\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from gensim.models import Word2Vec \n",
    "import sys\n",
    "\n",
    "# ==========================================\n",
    "# 0. Configuration\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random Seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ‡πÄ‡∏ä‡πá‡∏Ñ Path ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏±‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö\n",
    "CSV_PATH = r\"d:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_test.csv\" \n",
    "W2V_PATH = \"custom_word2vec.model\"\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 50       \n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "THRESHOLD = 0.5 \n",
    "\n",
    "# LSTM Specific Configs\n",
    "MAX_LEN = 200         \n",
    "EMBED_DIM = 300       \n",
    "HIDDEN_DIM = 256      \n",
    "NUM_LAYERS = 2        \n",
    "BIDIRECTIONAL = True  \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760c6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data & Building Vocabulary ---\n",
      "-> Data resources loaded.\n",
      "Vocab Size: 84551\n",
      "Converting text to Sequence IDs...\n",
      "Data Ready for LSTM!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Loading Data & Build Vocab\n",
    "# ==========================================\n",
    "print(\"--- Step 1: Loading Data & Building Vocabulary ---\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    w2v_model = Word2Vec.load(W2V_PATH)\n",
    "    print(\"-> Data resources loaded.\")\n",
    "except:\n",
    "    raise FileNotFoundError(f\"Check your file paths! \\nCSV: {CSV_PATH}\\nW2V: {W2V_PATH}\")\n",
    "\n",
    "# Build Embedding Matrix\n",
    "vocab = w2v_model.wv.key_to_index\n",
    "word_vectors = w2v_model.wv.vectors\n",
    "pad_vector = np.zeros((1, EMBED_DIM))  # ID 0\n",
    "unk_vector = np.random.normal(scale=0.6, size=(1, EMBED_DIM)) # ID 1\n",
    "final_embeddings = np.concatenate([pad_vector, unk_vector, word_vectors], axis=0)\n",
    "embedding_tensor = torch.FloatTensor(final_embeddings)\n",
    "\n",
    "print(f\"Vocab Size: {len(vocab) + 2}\")\n",
    "\n",
    "# Preprocessing Function\n",
    "stop_words = set(thai_stopwords())\n",
    "\n",
    "def text_to_indices(text, max_len=MAX_LEN):\n",
    "    tokens = word_tokenize(str(text), engine='newmm')\n",
    "    indices = []\n",
    "    for word in tokens:\n",
    "        if word.strip() == '' or word in stop_words: continue\n",
    "        if word in vocab:\n",
    "            indices.append(vocab[word] + 2) \n",
    "        else:\n",
    "            indices.append(1) \n",
    "            \n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    return indices\n",
    "\n",
    "print(\"Converting text to Sequence IDs...\")\n",
    "X_list = df['body_text'].apply(text_to_indices).tolist()\n",
    "X_numpy = np.array(X_list)\n",
    "X_tensor = torch.LongTensor(X_numpy).to(device)\n",
    "\n",
    "label_cols = ['politics', 'human_rights', 'quality_of_life', 'international', \n",
    "              'social', 'environment', 'economics', 'culture', 'labor', \n",
    "              'national_security', 'ict', 'education']\n",
    "y_numpy = df[label_cols].values\n",
    "y_tensor = torch.FloatTensor(y_numpy).to(device)\n",
    "num_classes = len(label_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Data Ready for LSTM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d908a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextLSTM(\n",
      "  (embedding): Embedding(84551, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. TextLSTM Model Definition\n",
    "# ==========================================\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        \n",
    "        # 1. Embedding\n",
    "        num_vocab, embed_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # 3. Fully Connected\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # output, (hidden, cell)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Concat hidden states from both directions\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n",
    "model = TextLSTM(embedding_tensor, HIDDEN_DIM, num_classes, NUM_LAYERS, BIDIRECTIONAL, 0.5).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95218cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nawapol\\anaconda3\\envs\\PT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training TextLSTM (50 Epochs) ---\n",
      "Epoch [5/50] Loss: 0.2090 | F1: 62.12% (Best: 62.12%)\n",
      "Epoch [10/50] Loss: 0.1512 | F1: 65.98% (Best: 65.98%)\n",
      "Epoch [15/50] Loss: 0.1099 | F1: 66.12% (Best: 66.12%)\n",
      "Epoch [20/50] Loss: 0.0792 | F1: 66.62% (Best: 66.62%)\n",
      "Epoch [25/50] Loss: 0.0570 | F1: 66.03% (Best: 66.62%)\n",
      "Epoch [30/50] Loss: 0.0501 | F1: 66.45% (Best: 66.62%)\n",
      "Epoch [35/50] Loss: 0.0368 | F1: 65.88% (Best: 66.62%)\n",
      "Epoch [40/50] Loss: 0.0304 | F1: 65.60% (Best: 66.62%)\n",
      "Epoch [45/50] Loss: 0.0238 | F1: 66.88% (Best: 66.88%)\n",
      "Epoch [50/50] Loss: 0.0204 | F1: 66.56% (Best: 66.88%)\n",
      "Loading Best Model from Epoch 45...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. Training\n",
    "# ==========================================\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"--- Training TextLSTM ({NUM_EPOCHS} Epochs) ---\")\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            probs = torch.sigmoid(test_outputs)\n",
    "            predicted = (probs > THRESHOLD).float()\n",
    "            current_f1 = f1_score(y_test.cpu().numpy(), predicted.cpu().numpy(), average='micro')\n",
    "            \n",
    "            if current_f1 > best_f1:\n",
    "                best_f1 = current_f1\n",
    "                best_epoch = epoch + 1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_lstm_model.pth')\n",
    "                \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Loss: {total_loss/len(train_loader):.4f} | F1: {current_f1*100:.2f}% (Best: {best_f1*100:.2f}%)\")\n",
    "\n",
    "print(f\"Loading Best Model from Epoch {best_epoch}...\")\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8195d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification Report (TextLSTM) ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         politics       0.76      0.83      0.79       766\n",
      "     human_rights       0.58      0.65      0.61       306\n",
      "  quality_of_life       0.61      0.63      0.62       211\n",
      "    international       0.73      0.78      0.76       158\n",
      "           social       0.37      0.23      0.29       171\n",
      "      environment       0.70      0.67      0.68       150\n",
      "        economics       0.58      0.64      0.61        94\n",
      "          culture       0.50      0.56      0.53        94\n",
      "            labor       0.81      0.81      0.81        69\n",
      "national_security       0.44      0.43      0.44        60\n",
      "              ict       0.64      0.75      0.69        65\n",
      "        education       0.50      0.43      0.46        56\n",
      "\n",
      "        micro avg       0.66      0.68      0.67      2200\n",
      "        macro avg       0.60      0.62      0.61      2200\n",
      "     weighted avg       0.65      0.68      0.66      2200\n",
      "      samples avg       0.68      0.69      0.66      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. Evaluation\n",
    "# ==========================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    probs = torch.sigmoid(test_outputs)\n",
    "    predicted = (probs > THRESHOLD).float()\n",
    "    \n",
    "    y_true = y_test.cpu().numpy()\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    \n",
    "    print(\"\\n--- Classification Report (TextLSTM) ---\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20051ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to stop.\n",
      "\n",
      "Snippet: 17 ‡∏û.‡∏¢. 2558 Blognone [1] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå ...\n",
      "[/] international: 99.89% (YES)\n",
      "[/] ict: 99.87% (YES)\n",
      "\n",
      "Snippet: ‡∏Å‡∏≠.‡∏£‡∏°‡∏ô.‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏Ñ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä...\n",
      "[/] politics: 98.62% (YES)\n",
      "[ ] economics: 15.22%\n",
      "[/] national_security: 96.72% (YES)\n",
      "[/] ict: 98.39% (YES)\n",
      "\n",
      "Snippet: ‡πÜ...\n",
      "[/] quality_of_life: 94.63% (YES)\n",
      "[ ] social: 21.18%\n",
      "[/] environment: 97.98% (YES)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. Interactive Mode\n",
    "# ==========================================\n",
    "def predict_lstm(text):\n",
    "    model.eval()\n",
    "    indices = text_to_indices(text) \n",
    "    tensor = torch.LongTensor([indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "        \n",
    "    print(f\"\\nSnippet: {text[:50]}...\")\n",
    "    found = False\n",
    "    for i, col in enumerate(label_cols):\n",
    "        if probs[i] > THRESHOLD:\n",
    "            print(f\"[/] {col}: {probs[i]*100:.2f}% (YES)\")\n",
    "            found = True\n",
    "        elif probs[i] > 0.15:\n",
    "            print(f\"[ ] {col}: {probs[i]*100:.2f}%\")\n",
    "    if not found: print(\">> No category detected.\")\n",
    "\n",
    "print(\"Type 'exit' to stop.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\\nüìù Enter news (TextLSTM): \").strip()\n",
    "        if user_input.lower() in ['exit', 'quit', 'q']: break\n",
    "        if not user_input: continue\n",
    "        predict_lstm(user_input)\n",
    "    except KeyboardInterrupt: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
