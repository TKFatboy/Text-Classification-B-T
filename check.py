import pandas as pd
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
from gensim.models import Word2Vec

# ==========================================
# 1. ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß (Input Text)
# ==========================================
news_text = """
17 ‡∏û.‡∏¢. 2558 Blognone [1] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anonymous ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏Å‡∏±‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏±‡∏ß‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏° IS ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏Å‡∏£‡∏∏‡∏á‡∏õ‡∏≤‡∏£‡∏µ‡∏™‡πÉ‡∏ô‡∏Ñ‡∏∑‡∏ô‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤

‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏õ‡πÉ‡∏ô YouTube ‡πÇ‡∏Ü‡∏©‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå‡∏™‡∏ß‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏î‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ñ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™ ‡∏°‡∏µ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤ ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° IS ‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡∏õ‡∏≤‡∏£‡∏µ‡∏™ ‡∏Å‡∏•‡∏∏‡πà‡∏° Anonymous ‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å‡∏à‡∏∞‡∏ï‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏° IS ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå Charlie Hebdo ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° Anonymous ‡πÄ‡∏•‡∏¢ ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏Å‡∏•‡∏∏‡πà‡∏° Anonymous ‡∏¢‡∏±‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏ú‡∏π‡πâ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ
‡∏Å‡∏•‡∏∏‡πà‡∏° Anonymous ‡πÄ‡∏Ñ‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏° IS ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå Charlie Hebdo ‡∏ó‡∏µ‡πà‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° Anonymous ‡∏≠‡πâ‡∏≤‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏á‡∏±‡∏ö‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö IS ‡πÑ‡∏õ‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ (‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏à‡∏≤‡∏ÅBlognone ‡∏ó‡∏µ‡πà  ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anonymous ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏Å‡∏ß‡∏≤‡∏î‡∏•‡πâ‡∏≤‡∏á‡∏û‡∏ß‡∏Å ISIS [2])
"""

print(f"--- ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö ({len(news_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ---")
print(news_text.strip()[:100] + "...") # ‡πÇ‡∏ä‡∏ß‡πå‡πÅ‡∏Ñ‡πà‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏û‡∏≠

# ==========================================
# 2. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• & Stopwords
# ==========================================
print("\nLoading Custom Word2Vec...")
try:
    model = Word2Vec.load("custom_word2vec.model")
    print("-> ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except:
    print("Error: ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå custom_word2vec.model")
    exit()

stop_words = set(thai_stopwords())
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏Ç‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á (Custom Stopwords)
custom_stops = {' ', '\n', '\t', '‚Äú', '‚Äù', '(', ')', '[', ']', '-', '.', ',', '1', '2'}
stop_words.update(custom_stops)

# ==========================================
# 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (The Inspection)
# ==========================================
print("\n--- ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Tokenization & Vocabulary ---")

# 3.1 ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (engine='newmm')
tokens = word_tokenize(news_text, engine='newmm')

valid_tokens = []      # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ)
unknown_tokens = []    # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (‡∏´‡∏≤‡∏¢‡∏ô‡∏∞!)
stopped_tokens = []    # ‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á (‡∏õ‡∏Å‡∏ï‡∏¥)

for word in tokens:
    word_clean = word.strip()
    if word_clean == '': continue # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    
    if word in stop_words:
        stopped_tokens.append(word)
    elif word in model.wv.key_to_index:
        # **‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç** ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏´‡∏°
        valid_tokens.append(word)
    else:
        # ‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô 0)
        unknown_tokens.append(word)

# ==========================================
# 4. ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•
# ==========================================
print(f"\n[‡∏™‡∏£‡∏∏‡∏õ] ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î: {len(tokens)}")
print(f"‚úÖ ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (Valid): {len(valid_tokens)} ‡∏Ñ‡∏≥")
print(f"‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (Unknown): {len(unknown_tokens)} ‡∏Ñ‡∏≥")
print(f"üóëÔ∏è ‡∏Ñ‡∏≥‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á (Stopwords): {len(stopped_tokens)} ‡∏Ñ‡∏≥")

print("\n-------------------------------------------------------------")
print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Keyword ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏£‡∏≠‡∏î):")
print(f"Valid Tokens (‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô): {valid_tokens[:150]} ...")
print("-------------------------------------------------------------")

# ‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
target_keywords = ['Anonymous', '‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå', 'IS', '‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°', '‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå', 'YouTube', 'Charlie', 'Hebdo']
print("\nüîç ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÄ‡∏ä‡πá‡∏Ñ Keyword ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:")
for kw in target_keywords:
    status = "‚úÖ ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å" if kw in model.wv.key_to_index else "‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (‡∏´‡∏≤‡∏¢!)"
    print(f"   - '{kw}' : {status}")

print("\n-------------------------------------------------------------")
print("‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (Unknown Words) - ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏´‡∏°:")
print(unknown_tokens)