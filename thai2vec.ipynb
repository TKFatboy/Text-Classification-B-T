{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ab0d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch is running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp import word_vector\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# Check Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ PyTorch is running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a644a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Thai2Vec model (thai2fit_wv)...\n",
      "‚úÖ Thai2Vec Loaded successfully.\n",
      "‚úÖ CSV Loaded: 6789 records\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Thai2Vec\n",
    "print(\"‚è≥ Loading Thai2Vec model (thai2fit_wv)...\")\n",
    "try:\n",
    "    wv_wrapper = word_vector.WordVector(model_name=\"thai2fit_wv\")\n",
    "    wv = wv_wrapper.get_model() \n",
    "    print(\"‚úÖ Thai2Vec Loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "\n",
    "# 2. Load CSV\n",
    "file_name = r\"d:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_test.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(f\"‚úÖ CSV Loaded: {len(df)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a782c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function ready.\n"
     ]
    }
   ],
   "source": [
    "# Stopwords Setup\n",
    "stop_words = set(thai_stopwords())\n",
    "my_custom_stops = {' ', '\\n', '\\t', '‚Äú', '‚Äù', '(', ')', '[', ']', '-', '.', ',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "stop_words.update(my_custom_stops)\n",
    "\n",
    "# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ ‡πÅ‡∏•‡∏∞ ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏î‡πâ‡∏ß‡∏¢ ---\n",
    "def get_word_vectors_and_tokens(text):\n",
    "    tokens = word_tokenize(str(text), engine='newmm')\n",
    "    \n",
    "    vecs = []\n",
    "    kept_tokens = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and word.strip() != '':\n",
    "            try:\n",
    "                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á Vector ‡∏à‡∏≤‡∏Å Thai2Fit\n",
    "                vec = wv.get_vector(word) \n",
    "                vecs.append(vec)\n",
    "                kept_tokens.append(word) # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß\n",
    "            except:\n",
    "                pass # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Dict ‡∏Å‡πá‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ\n",
    "    \n",
    "    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\n",
    "    if len(vecs) == 0:\n",
    "        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Matrix ‡∏ß‡πà‡∏≤‡∏á (size 0, 300) ‡πÅ‡∏•‡∏∞ List ‡∏ß‡πà‡∏≤‡∏á\n",
    "        return np.zeros((0, wv.vector_size)), [] \n",
    "    \n",
    "    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 2 ‡∏≠‡∏¢‡πà‡∏≤‡∏á: (Matrix ‡∏Ç‡∏≠‡∏á Vector, List ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô)\n",
    "    return np.array(vecs), kept_tokens\n",
    "\n",
    "print(\"‚úÖ Function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8860994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting text to Word Matrices and Tokens...\n",
      "‚úÖ Processing Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>final_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anonymous ‡∏•‡∏±‡πà‡∏ô‡∏ó‡∏≥‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏ç...</td>\n",
       "      <td>[17, ‡∏û, ‡∏¢, 2558, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°, ‡∏´‡∏±‡∏ß‡∏£‡∏∏‡∏ô‡πÅ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∏™‡∏ï‡∏π‡∏î‡∏¥‡πÇ‡∏≠‡∏à‡∏¥‡∏ö‡∏•‡∏¥‡∏ï‡πâ‡∏≤‡∏ô‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏° ‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç...</td>\n",
       "      <td>[‡∏™.‡∏™., ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô, ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£, ‡πÅ‡∏Å‡πâ, ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç, ‡∏Å‡∏≠‡∏á‡∏Å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We need Safety Zone ‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏ô‡∏≥‡πÄ‡∏î‡∏¥‡∏ô‡∏£‡∏ì...</td>\n",
       "      <td>[‡∏™‡∏°‡∏≤‡∏Ñ‡∏°, ‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û, ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢, ‡πÄ‡∏î‡πá‡∏Å, ‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô, ‡∏õ‡∏£‡∏∞...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  ‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå Anonymous ‡∏•‡∏±‡πà‡∏ô‡∏ó‡∏≥‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏ã‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏ç...   \n",
       "1  ‡∏™‡∏ï‡∏π‡∏î‡∏¥‡πÇ‡∏≠‡∏à‡∏¥‡∏ö‡∏•‡∏¥‡∏ï‡πâ‡∏≤‡∏ô‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏° ‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç...   \n",
       "2  We need Safety Zone ‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏ô‡∏≥‡πÄ‡∏î‡∏¥‡∏ô‡∏£‡∏ì...   \n",
       "\n",
       "                                        final_tokens  \n",
       "0  [17, ‡∏û, ‡∏¢, 2558, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°, ‡∏´‡∏±‡∏ß‡∏£‡∏∏‡∏ô‡πÅ...  \n",
       "1  [‡∏™.‡∏™., ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô, ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£, ‡πÅ‡∏Å‡πâ, ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç, ‡∏Å‡∏≠‡∏á‡∏Å...  \n",
       "2  [‡∏™‡∏°‡∏≤‡∏Ñ‡∏°, ‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û, ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢, ‡πÄ‡∏î‡πá‡∏Å, ‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô, ‡∏õ‡∏£‡∏∞...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà 5 ‡πÅ‡∏ñ‡∏ß\n",
    "df_small = df.head(5).copy()\n",
    "\n",
    "print(\"üîÑ Converting text to Word Matrices and Tokens...\")\n",
    "\n",
    "# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß\n",
    "results = df_small['body_text'].apply(get_word_vectors_and_tokens)\n",
    "\n",
    "# ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "df_small['word_matrices'] = results.apply(lambda x: x[0]) # ‡πÄ‡∏Å‡πá‡∏ö Matrix (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)\n",
    "df_small['final_tokens']  = results.apply(lambda x: x[1]) # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥ (Text)\n",
    "\n",
    "print(\"‚úÖ Processing Done.\")\n",
    "display(df_small[['title', 'final_tokens']].head(3)) # ‡πÇ‡∏ä‡∏ß‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002f7ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1 ---\n",
      "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î Stopwords: 59 ‡∏Ñ‡∏≥\n",
      "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix (‡∏Ñ‡∏≥ x 300): (59, 300)\n",
      "\n",
      "--- üìù ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥ (Tokens) ---\n",
      "['17', '‡∏û', '‡∏¢', '2558', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°', '‡∏´‡∏±‡∏ß‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á', '‡∏≠‡∏≠‡∏Å‡∏°‡∏≤', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏ú‡∏π‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á', '‡πÇ‡∏à‡∏°‡∏ï‡∏µ', '‡∏Å‡∏£‡∏∏‡∏á', '‡∏õ‡∏≤‡∏£‡∏µ‡∏™', '‡∏Ñ‡∏∑‡∏ô', '‡∏®‡∏∏‡∏Å‡∏£‡πå', '‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤', '‡∏†‡∏≤‡∏û', '‡∏Ñ‡∏•‡∏¥‡∏õ', '‡πÇ‡∏Ü‡∏©‡∏Å', '‡∏™‡∏ß‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å', '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå', '‡∏≠‡∏≠‡∏Å‡∏°‡∏≤', '‡∏≠‡πà‡∏≤‡∏ô', '‡πÅ‡∏ñ‡∏•‡∏á', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™', '‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°', '‡πÇ‡∏à‡∏°‡∏ï‡∏µ', '‡∏Å‡∏£‡∏∏‡∏á', '‡∏õ‡∏≤‡∏£‡∏µ‡∏™', '‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å', '‡∏ï‡∏≤‡∏°‡∏•‡πà‡∏≤', '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô', '‡∏ó‡∏≥', '‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà', '‡πÇ‡∏à‡∏°‡∏ï‡∏µ', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå', '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£', '‡πÇ‡∏à‡∏°‡∏ï‡∏µ', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à', '‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', '‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢', '‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°', '‡πÇ‡∏à‡∏°‡∏ï‡∏µ', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå', '‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™', '‡∏ï‡πâ‡∏ô‡∏õ‡∏µ', '‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤', '‡∏≠‡πâ‡∏≤‡∏á‡∏ß‡πà‡∏≤']\n",
      "\n",
      "--- üî¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Vector ---\n",
      "‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤: '17'\n",
      "Vector (20 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å): [ 2.17996e-01  4.87145e-01  7.85160e-02  2.91262e-01 -1.14988e-01\n",
      " -3.57300e-02  5.92545e-01  1.00819e-01 -1.36525e-01 -6.82950e-02\n",
      " -2.27718e-01 -1.90961e-01 -2.21790e-02  2.20321e-01  2.39987e-01\n",
      "  1.36656e-01  1.90440e-02  3.32210e-02 -1.85260e-02  5.58000e-04]\n",
      "...\n",
      "\n",
      "‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤: '‡∏û'\n",
      "Vector (20 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å): [-0.435066  0.216454 -0.054543 -0.178221  0.015009  0.332824 -0.092603\n",
      " -0.035642  0.008967 -0.033077 -0.325049 -0.180303  0.153345 -0.378494\n",
      " -0.095105  0.13991   0.424235 -0.253541 -0.008633 -0.248073]\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# üîç ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡∏π‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ!)\n",
    "idx = 0\n",
    "# ===========================\n",
    "\n",
    "if idx < len(df_small):\n",
    "    matrix_news = df_small['word_matrices'].iloc[idx]\n",
    "    tokens_news = df_small['final_tokens'].iloc[idx]\n",
    "\n",
    "    print(f\"--- üì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {idx+1} ---\")\n",
    "    print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î Stopwords: {len(tokens_news)} ‡∏Ñ‡∏≥\")\n",
    "    print(f\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix (‡∏Ñ‡∏≥ x 300): {matrix_news.shape}\") \n",
    "\n",
    "    print(\"\\n--- üìù ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥ (Tokens) ---\")\n",
    "    print(tokens_news[:50]) # ‡πÇ‡∏ä‡∏ß‡πå 50 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å\n",
    "\n",
    "    print(\"\\n--- üî¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Vector ---\")\n",
    "    if len(matrix_news) > 0:\n",
    "        print(f\"‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤: '{tokens_news[0]}'\") \n",
    "        print(f\"Vector (20 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å): {matrix_news[0][:20]}\")\n",
    "        print(\"...\")\n",
    "        print(f\"\\n‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤: '{tokens_news[1]}'\") \n",
    "        print(f\"Vector (20 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å): {matrix_news[1][:20]}\")\n",
    "else:\n",
    "    print(\"‚ùå Index ‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
