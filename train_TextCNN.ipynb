{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ad93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "# ==========================================\n",
    "# 0. Configuration\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random Seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "CSV_PATH = r\"d:\\year4\\‡∏™‡∏´‡∏Å‡∏¥‡∏à\\prachatai_test.csv\"\n",
    "W2V_PATH = \"custom_word2vec.model\"\n",
    "NUM_EPOCHS = 50 # CNN ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ MLP ‡πÄ‡∏¢‡∏≠‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö 50-100 ‡∏Å‡πá‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "THRESHOLD = 0.5 \n",
    "\n",
    "# CNN Configs\n",
    "MAX_LEN = 200         # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà 200 ‡∏Ñ‡∏≥)\n",
    "EMBED_DIM = 300       # ‡∏Ç‡∏ô‡∏≤‡∏î Word2Vec ‡πÄ‡∏î‡∏¥‡∏°\n",
    "NUM_FILTERS = 100     # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Pattern ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î\n",
    "FILTER_SIZES = [2, 3, 4] # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Pattern ‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á (2 ‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô, 3 ‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô, 4 ‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Loading Data & Build Vocab\n",
    "# ==========================================\n",
    "print(\"--- Step 1: Loading Data & Building Vocabulary ---\")\n",
    "\n",
    "# 1.1 Load Raw Data\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    w2v_model = Word2Vec.load(W2V_PATH)\n",
    "    print(\"-> Data resources loaded.\")\n",
    "except:\n",
    "    raise FileNotFoundError(\"Check your file paths!\")\n",
    "\n",
    "# 1.2 Build Embedding Matrix (‡πÅ‡∏õ‡∏•‡∏á Word2Vec ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡πâ CNN ‡πÉ‡∏ä‡πâ)\n",
    "# ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 0 ‡∏Ñ‡∏∑‡∏≠ <PAD>, ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1 ‡∏Ñ‡∏∑‡∏≠ <UNK>, ‡πÅ‡∏ñ‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡πÉ‡∏ô Word2Vec\n",
    "vocab = w2v_model.wv.key_to_index\n",
    "word_vectors = w2v_model.wv.vectors\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° <PAD> ‡πÅ‡∏•‡∏∞ <UNK> ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n",
    "pad_vector = np.zeros((1, EMBED_DIM))  # ID 0: ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏á‡πÜ (‡∏™‡∏µ‡∏î‡∏≥)\n",
    "unk_vector = np.random.normal(scale=0.6, size=(1, EMBED_DIM)) # ID 1: ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å\n",
    "\n",
    "# ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á: [PAD, UNK, ...Word2Vec...]\n",
    "final_embeddings = np.concatenate([pad_vector, unk_vector, word_vectors], axis=0)\n",
    "embedding_tensor = torch.FloatTensor(final_embeddings)\n",
    "\n",
    "print(f\"Vocab Size: {len(vocab) + 2}\")\n",
    "print(f\"Embedding Matrix Shape: {embedding_tensor.shape}\")\n",
    "\n",
    "# 1.3 Preprocessing Function (Text -> List of IDs)\n",
    "stop_words = set(thai_stopwords())\n",
    "\n",
    "def text_to_indices(text, max_len=MAX_LEN):\n",
    "    tokens = word_tokenize(str(text), engine='newmm')\n",
    "    indices = []\n",
    "    for word in tokens:\n",
    "        if word.strip() == '' or word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        if word in vocab:\n",
    "            # +2 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏°‡∏µ PAD(0) ‡∏Å‡∏±‡∏ö UNK(1) ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà\n",
    "            indices.append(vocab[word] + 2) \n",
    "        else:\n",
    "            indices.append(1) # Unknown\n",
    "            \n",
    "    # Padding / Truncating (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà MAX_LEN)\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices)) # ‡πÄ‡∏ï‡∏¥‡∏° 0 ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡πá‡∏°\n",
    "    else:\n",
    "        indices = indices[:max_len] # ‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô\n",
    "        \n",
    "    return indices\n",
    "\n",
    "# 1.4 Convert All Data\n",
    "print(\"Converting text to Sequence IDs...\")\n",
    "X_list = df['body_text'].apply(text_to_indices).tolist()\n",
    "X_numpy = np.array(X_list)\n",
    "X_tensor = torch.LongTensor(X_numpy).to(device) # ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï! ‡πÄ‡∏õ‡πá‡∏ô LongTensor (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°)\n",
    "\n",
    "# Labels (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)\n",
    "label_cols = ['politics', 'human_rights', 'quality_of_life', 'international', \n",
    "              'social', 'environment', 'economics', 'culture', 'labor', \n",
    "              'national_security', 'ict', 'education']\n",
    "y_numpy = df[label_cols].values\n",
    "y_tensor = torch.FloatTensor(y_numpy).to(device)\n",
    "num_classes = len(label_cols)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Data Ready for CNN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. TextCNN Model Definition\n",
    "# ==========================================\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes, filter_sizes, num_filters):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer: ‡πÇ‡∏´‡∏•‡∏î Word2Vec ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False) \n",
    "        # freeze=False ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ Word2Vec ‡∏Ç‡∏¢‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏£‡∏≤\n",
    "        \n",
    "        # 2. Conv Layers: ‡∏ï‡∏±‡∏ß‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤ Pattern (‡πÄ‡∏ä‡πà‡∏ô \"‡∏´‡∏¢‡∏∏‡∏î-‡∏á‡∏≤‡∏ô\")\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, \n",
    "                      out_channels=num_filters, \n",
    "                      kernel_size=fs) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # 3. Dropout & Linear Output\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, max_len] (‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç ID)\n",
    "        \n",
    "        x = self.embedding(x) \n",
    "        # x shape: [batch, max_len, embed_dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1) \n",
    "        # x shape: [batch, embed_dim, max_len] (‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏•‡∏±‡∏ö‡πÅ‡∏Å‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Conv1d ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ)\n",
    "        \n",
    "        # Apply Convolution + ReLU + MaxPool\n",
    "        conved = [F.relu(conv(x)) for conv in self.convs]\n",
    "        \n",
    "        # Max Pooling over time (‡∏´‡∏≤ Feature ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å Filter (2‡∏Ñ‡∏≥, 3‡∏Ñ‡∏≥, 4‡∏Ñ‡∏≥) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        \n",
    "        out = self.dropout(cat)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = TextCNN(embedding_tensor, num_classes, FILTER_SIZES, NUM_FILTERS).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Training\n",
    "# ==========================================\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"--- Training TextCNN ({NUM_EPOCHS} Epochs) ---\")\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    if (epoch+1) % 5 == 0: # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ö‡πà‡∏≠‡∏¢‡∏´‡∏ô‡πà‡∏≠‡∏¢\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            probs = torch.sigmoid(test_outputs)\n",
    "            predicted = (probs > THRESHOLD).float()\n",
    "            current_f1 = f1_score(y_test.cpu().numpy(), predicted.cpu().numpy(), average='micro')\n",
    "            \n",
    "            if current_f1 > best_f1:\n",
    "                best_f1 = current_f1\n",
    "                best_epoch = epoch + 1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_cnn_model.pth')\n",
    "                \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Loss: {total_loss/len(train_loader):.4f} | F1: {current_f1*100:.2f}% (Best: {best_f1*100:.2f}%)\")\n",
    "\n",
    "print(f\"Loading Best Model from Epoch {best_epoch}...\")\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2082830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Evaluation\n",
    "# ==========================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    probs = torch.sigmoid(test_outputs)\n",
    "    predicted = (probs > THRESHOLD).float()\n",
    "    \n",
    "    y_true = y_test.cpu().numpy()\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    \n",
    "    print(\"\\n--- Classification Report (TextCNN) ---\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Interactive Mode\n",
    "# ==========================================\n",
    "def predict_cnn(text):\n",
    "    model.eval()\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á text ‡πÄ‡∏õ‡πá‡∏ô ID sequence\n",
    "    indices = text_to_indices(text) \n",
    "    tensor = torch.LongTensor([indices]).to(device) # ‡πÉ‡∏™‡πà [] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥ Batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "        \n",
    "    print(f\"\\nSnippet: {text[:50]}...\")\n",
    "    found = False\n",
    "    for i, col in enumerate(label_cols):\n",
    "        if probs[i] > THRESHOLD:\n",
    "            print(f\"[/] {col}: {probs[i]*100:.2f}% (YES)\")\n",
    "            found = True\n",
    "        elif probs[i] > 0.15:\n",
    "            print(f\"[ ] {col}: {probs[i]*100:.2f}%\")\n",
    "    if not found: print(\">> No category detected.\")\n",
    "\n",
    "print(\"Type 'exit' to stop.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\\nüìù Enter news (TextCNN): \").strip()\n",
    "        if user_input.lower() in ['exit', 'quit', 'q']: break\n",
    "        if not user_input: continue\n",
    "        predict_cnn(user_input)\n",
    "    except KeyboardInterrupt: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
