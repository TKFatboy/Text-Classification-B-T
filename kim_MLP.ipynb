{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e094134a-a458-47b1-afc5-d6657df633f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pythainlp\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8335dd-88d8-4720-a6e5-83b84805e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏â‡∏±‡∏ô', '‡∏´‡∏¥‡∏ß‡∏Ç‡πâ‡∏≤‡∏ß', '‡∏°‡∏≤‡∏Å']\n"
     ]
    }
   ],
   "source": [
    "import pythainlp\n",
    "print(pythainlp.word_tokenize(\"‡∏â‡∏±‡∏ô‡∏´‡∏¥‡∏ß‡∏Ç‡πâ‡∏≤‡∏ß‡∏°‡∏≤‡∏Å\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3f81f1-091c-4d4b-aec8-2bae1f3b78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcf9b4f-b587-4366-91a4-b548f284daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏Ç‡πâ‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô', '‡∏ä‡∏¥‡πâ‡∏ô', '‡∏ô‡∏µ‡πâ', ' ', '‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£', '‡∏û‡∏π‡∏î', '‡∏Å‡∏±‡∏ô', '‡∏≠‡∏¢‡πà‡∏≤‡∏á', '‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤', '‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î']\n"
     ]
    }
   ],
   "source": [
    "import pythainlp\n",
    "print(pythainlp.word_tokenize(\"‡∏Ç‡πâ‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d16a4-9739-4bf5-b384-79bba9ae0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install spacy fastcoref pythainlp transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa549f8-b76e-4525-b9a0-8bbb581585c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "nlp = spacy.blank(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d2f13d-f172-4234-b18f-46d145548a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\anaconda\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub[hf_xet])\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "Installing collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec11c79-91ea-4f50-b8de-7bf21dae628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "01/06/2026 15:15:26 - WARNING - \t Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac203a6e3bb4412a0ba04a75b470dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  24%|##3       | 755M/3.15G [00:03<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:15:58 - INFO - \t missing_keys: []\n",
      "01/06/2026 15:15:58 - INFO - \t unexpected_keys: []\n",
      "01/06/2026 15:15:58 - INFO - \t mismatched_keys: []\n",
      "01/06/2026 15:15:58 - INFO - \t error_msgs: []\n",
      "01/06/2026 15:15:58 - INFO - \t Model Parameters: 786.9M, Transformer: 778.5M, Coref head: 8.4M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastcoref.spacy_component.spacy_component.FastCorefResolver at 0x18b6108ba10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\n",
    "   \"fastcoref\",\n",
    "   config={'model_architecture': 'FCoref','model_path': 'pythainlp/han-coref-v1.0'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbec789-98fa-44e4-979f-3c33437859a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get2tag(tags,text,title=None):\n",
    "    dic_ents = {\"text\":text,\"ents\":[],\"title\":title}\n",
    "    _tag=[str(i) for i in list(range(len(tags)))]\n",
    "    for i,tag in enumerate(tags):\n",
    "        for s,e in tag:\n",
    "            dic_ents[\"ents\"].append({\"start\": s, \"end\": e, \"label\": _tag[i]})\n",
    "    colors={i:\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in _tag} # thank https://stackoverflow.com/a/50218895\n",
    "    return dic_ents,{\"colors\":colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41f56c52-7a7b-4dce-83eb-51489718d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58d4770-0558-435c-99cc-169551066a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:17:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dc2c63768c4172803546511116995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:17:06 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ef584f464b4dfaa004d8515f37d025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #308774; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0</span>\n",
       "</mark>\n",
       " ‡πÅ‡∏à‡∏á‡∏ß‡∏∏‡πà‡∏ô ‡∏†‡∏≤‡∏û‡πÅ‡∏Ñ‡∏õ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏•‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏Ø ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏¢ ‡∏û‡∏¥‡∏ò‡∏≤ ‡∏¢‡∏±‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà\n",
       "<mark class=\"entity\" style=\"background: #308774; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">0</span>\n",
       "</mark>\n",
       " ‡πÅ‡∏ï‡πà‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡πà‡∏≤‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏î‡πâ‡∏≠‡∏¢‡∏Ñ‡πà‡∏≤‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç ‡∏Ç‡∏£‡∏Å.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text=\"‡∏™‡∏≤‡∏ò‡∏¥‡∏ï ‡πÅ‡∏à‡∏á‡∏ß‡∏∏‡πà‡∏ô ‡∏†‡∏≤‡∏û‡πÅ‡∏Ñ‡∏õ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏•‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏Ø ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏¢ ‡∏û‡∏¥‡∏ò‡∏≤ ‡∏¢‡∏±‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡πà‡∏≤‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏î‡πâ‡∏≠‡∏¢‡∏Ñ‡πà‡∏≤‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç ‡∏Ç‡∏£‡∏Å.\"\n",
    "doc = nlp(text)\n",
    "dic_ents,colors=get2tag(doc._.coref_clusters,text)\n",
    "displacy.render(dic_ents, manual=True, style=\"ent\",options=colors, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d315b92-6428-4e57-83ba-1cabf958414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "success\n",
      "------------------------------\n",
      "‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 55160 ‡πÅ‡∏ñ‡∏ß\n",
      "  Unnamed: 0                                url              date  \\\n",
      "0          0   https://prachatai.com/print/7262  2006-02-09 05:58   \n",
      "1          1  https://prachatai.com/print/76570  2018-04-25 19:17   \n",
      "2          2  https://prachatai.com/print/17871  2008-08-28 18:32   \n",
      "3          3  https://prachatai.com/print/23418  2007-04-09 10:44   \n",
      "4          4  https://prachatai.com/print/35692  2011-06-26 16:41   \n",
      "\n",
      "                                               title  \\\n",
      "0                      ‡∏ß‡∏∏‡∏í‡∏¥‡∏™‡∏†‡∏≤‡∏à‡∏µ‡πâ‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏á‡πÇ‡∏õ‡πÅ‡∏ï‡∏ä   \n",
      "1  ‡∏à‡∏ô‡∏ó.‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πà‡∏á‡∏£‡∏∞‡∏á‡∏±‡∏ö‡∏™‡∏±‡∏°‡∏°‡∏ô‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≠‡∏á‡∏ó‡∏±‡∏û‡∏û‡∏°‡πà‡∏≤‡∏•‡∏∞‡πÄ‡∏°...   \n",
      "2  ‡∏õ‡∏£‡∏∞‡∏°‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏™‡∏á‡∏Ç‡∏•‡∏≤‡∏õ‡πâ‡∏≠‡∏á‡∏ó‡∏∞‡πÄ‡∏• ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ï‡πâ‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏à‡∏≤...   \n",
      "3    ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏Å \"‡∏Å‡∏•‡∏∏‡πà‡∏° FTA WATCH\"  ‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏•‡∏á‡∏ô‡∏≤‡∏° JTEPA   \n",
      "4  ‡∏ó‡∏±‡∏û‡πÑ‡∏ó‡πÉ‡∏´‡∏ç‡πà ‚Äú‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‚Äù ‡πÄ‡∏™‡∏µ‡∏¢‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏´‡πâ‡∏û‡∏°‡πà‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏´‡πà‡∏á...   \n",
      "\n",
      "                                           body_text  politics  human_rights  \\\n",
      "0  ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡πÑ‡∏ó¬ó9 ‡∏Å.‡∏û. 2549 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 8 ‡∏Å.‡∏û. ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞...       1.0           0.0   \n",
      "1  ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏™‡∏±‡πà‡∏á‡πÄ‡∏ö‡∏£‡∏Ñ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏•‡∏∞‡πÄ‡∏°‡∏¥...       0.0           1.0   \n",
      "2  26 ‡∏™.‡∏Ñ. 51 - ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏á 262 ‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡∏°...       1.0           0.0   \n",
      "3  ‡∏ò‡∏µ‡∏£‡∏°‡∏• ‡∏ö‡∏±‡∏ß‡∏á‡∏≤‡∏° ‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå/‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á\\n\\n‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏õ...       0.0           0.0   \n",
      "4  ‡∏Å‡∏≠‡∏á‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÑ‡∏ó‡πÉ‡∏´‡∏ç‡πà ‚Äú‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‚Äù ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ...       0.0           0.0   \n",
      "\n",
      "   quality_of_life  international  social  environment  economics  culture  \\\n",
      "0              0.0            0.0     0.0          0.0        0.0      0.0   \n",
      "1              0.0            1.0     1.0          0.0        0.0      0.0   \n",
      "2              0.0            0.0     0.0          0.0        0.0      0.0   \n",
      "3              0.0            0.0     0.0          0.0        0.0      0.0   \n",
      "4              0.0            1.0     0.0          0.0        0.0      0.0   \n",
      "\n",
      "   labor  national_security  ict  education  \n",
      "0    0.0                0.0  0.0        0.0  \n",
      "1    0.0                0.0  0.0        0.0  \n",
      "2    0.0                0.0  0.0        0.0  \n",
      "3    0.0                0.0  0.0        0.0  \n",
      "4    0.0                0.0  0.0        0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\prachatai_train.csv'\n",
    "\n",
    "try:\n",
    "    print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')\n",
    "    \n",
    "    print(\"success\")  \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(f\"‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î Error ‡∏ï‡∏≤‡∏°‡∏ô‡∏µ‡πâ:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dccc36d2-cfb0-45a4-a3a7-b024db485184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '0'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 2. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m subset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠ '0'\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà (\\n) ‡∏≠‡∏≠‡∏Å‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡πÜ\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "import pythainlp\n",
    "\n",
    "# 1. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 3 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å\n",
    "subset = df.head(3)\n",
    "\n",
    "print(\"--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å ---\")\n",
    "\n",
    "# 2. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß\n",
    "for index, row in subset.iterrows():\n",
    "    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠ '0'\n",
    "    text = str(row['0'])\n",
    "    \n",
    "    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà (\\n) ‡∏≠‡∏≠‡∏Å‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡πÜ\n",
    "    clean_text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # 3. ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥\n",
    "    words = pythainlp.word_tokenize(clean_text, engine='newmm')\n",
    "    \n",
    "    # 4. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•\n",
    "    print(f\"\\nüîπ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà {index+1}\")\n",
    "    print(f\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {clean_text[:100]}...\") # ‡πÇ‡∏ä‡∏ß‡πå‡πÅ‡∏Ñ‡πà‡∏ô‡∏¥‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠\n",
    "    print(f\"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {words}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109125de-6265-42b0-95c6-5bbc0071f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in d:\\anaconda\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 422.4 kB/s eta 0:00:57\n",
      "    --------------------------------------- 0.5/24.4 MB 422.4 kB/s eta 0:00:57\n",
      "    --------------------------------------- 0.5/24.4 MB 422.4 kB/s eta 0:00:57\n",
      "   - -------------------------------------- 0.8/24.4 MB 408.3 kB/s eta 0:00:58\n",
      "   - -------------------------------------- 0.8/24.4 MB 408.3 kB/s eta 0:00:58\n",
      "   - -------------------------------------- 0.8/24.4 MB 408.3 kB/s eta 0:00:58\n",
      "   - -------------------------------------- 0.8/24.4 MB 408.3 kB/s eta 0:00:58\n",
      "   - -------------------------------------- 1.0/24.4 MB 396.5 kB/s eta 0:00:59\n",
      "   - -------------------------------------- 1.0/24.4 MB 396.5 kB/s eta 0:00:59\n",
      "   - -------------------------------------- 1.0/24.4 MB 396.5 kB/s eta 0:00:59\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.3/24.4 MB 404.8 kB/s eta 0:00:58\n",
      "   -- ------------------------------------- 1.6/24.4 MB 327.4 kB/s eta 0:01:10\n",
      "   -- ------------------------------------- 1.6/24.4 MB 327.4 kB/s eta 0:01:10\n",
      "   -- ------------------------------------- 1.6/24.4 MB 327.4 kB/s eta 0:01:10\n",
      "   -- ------------------------------------- 1.6/24.4 MB 327.4 kB/s eta 0:01:10\n",
      "   -- ------------------------------------- 1.6/24.4 MB 327.4 kB/s eta 0:01:10\n",
      "   --- ------------------------------------ 1.8/24.4 MB 315.1 kB/s eta 0:01:12\n",
      "   --- ------------------------------------ 1.8/24.4 MB 315.1 kB/s eta 0:01:12\n",
      "   --- ------------------------------------ 1.8/24.4 MB 315.1 kB/s eta 0:01:12\n",
      "   --- ------------------------------------ 1.8/24.4 MB 315.1 kB/s eta 0:01:12\n",
      "   --- ------------------------------------ 2.1/24.4 MB 311.1 kB/s eta 0:01:12\n",
      "   --- ------------------------------------ 2.1/24.4 MB 311.1 kB/s eta 0:01:12\n",
      "   --- ------------------------------------ 2.4/24.4 MB 329.8 kB/s eta 0:01:07\n",
      "   --- ------------------------------------ 2.4/24.4 MB 329.8 kB/s eta 0:01:07\n",
      "   --- ------------------------------------ 2.4/24.4 MB 329.8 kB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 344.1 kB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 344.1 kB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 359.8 kB/s eta 0:01:00\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 359.8 kB/s eta 0:01:00\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 375.2 kB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 375.2 kB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 385.4 kB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 385.4 kB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 385.4 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.7/24.4 MB 387.0 kB/s eta 0:00:54\n",
      "   ------ --------------------------------- 3.7/24.4 MB 387.0 kB/s eta 0:00:54\n",
      "   ------ --------------------------------- 3.7/24.4 MB 387.0 kB/s eta 0:00:54\n",
      "   ------ --------------------------------- 3.7/24.4 MB 387.0 kB/s eta 0:00:54\n",
      "   ------ --------------------------------- 3.7/24.4 MB 387.0 kB/s eta 0:00:54\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 3.9/24.4 MB 372.9 kB/s eta 0:00:55\n",
      "   ------ --------------------------------- 4.2/24.4 MB 333.8 kB/s eta 0:01:01\n",
      "   ------- -------------------------------- 4.5/24.4 MB 347.6 kB/s eta 0:00:58\n",
      "   ------- -------------------------------- 4.7/24.4 MB 360.5 kB/s eta 0:00:55\n",
      "   ------- -------------------------------- 4.7/24.4 MB 360.5 kB/s eta 0:00:55\n",
      "   -------- ------------------------------- 5.0/24.4 MB 371.2 kB/s eta 0:00:53\n",
      "   -------- ------------------------------- 5.0/24.4 MB 371.2 kB/s eta 0:00:53\n",
      "   -------- ------------------------------- 5.2/24.4 MB 380.1 kB/s eta 0:00:51\n",
      "   --------- ------------------------------ 5.5/24.4 MB 390.4 kB/s eta 0:00:49\n",
      "   --------- ------------------------------ 5.5/24.4 MB 390.4 kB/s eta 0:00:49\n",
      "   --------- ------------------------------ 5.8/24.4 MB 398.6 kB/s eta 0:00:47\n",
      "   --------- ------------------------------ 5.8/24.4 MB 398.6 kB/s eta 0:00:47\n",
      "   --------- ------------------------------ 6.0/24.4 MB 404.9 kB/s eta 0:00:46\n",
      "   --------- ------------------------------ 6.0/24.4 MB 404.9 kB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 414.6 kB/s eta 0:00:44\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 414.6 kB/s eta 0:00:44\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 422.6 kB/s eta 0:00:43\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 431.3 kB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 431.3 kB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 437.5 kB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 437.5 kB/s eta 0:00:40\n",
      "   ------------ --------------------------- 7.3/24.4 MB 444.5 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 7.3/24.4 MB 444.5 kB/s eta 0:00:39\n",
      "   ------------ --------------------------- 7.6/24.4 MB 450.0 kB/s eta 0:00:38\n",
      "   ------------ --------------------------- 7.9/24.4 MB 458.7 kB/s eta 0:00:37\n",
      "   ------------- -------------------------- 8.1/24.4 MB 466.3 kB/s eta 0:00:35\n",
      "   ------------- -------------------------- 8.4/24.4 MB 474.9 kB/s eta 0:00:34\n",
      "   ------------- -------------------------- 8.4/24.4 MB 474.9 kB/s eta 0:00:34\n",
      "   -------------- ------------------------- 8.7/24.4 MB 483.1 kB/s eta 0:00:33\n",
      "   -------------- ------------------------- 8.9/24.4 MB 491.1 kB/s eta 0:00:32\n",
      "   --------------- ------------------------ 9.2/24.4 MB 498.2 kB/s eta 0:00:31\n",
      "   --------------- ------------------------ 9.2/24.4 MB 498.2 kB/s eta 0:00:31\n",
      "   --------------- ------------------------ 9.4/24.4 MB 505.5 kB/s eta 0:00:30\n",
      "   --------------- ------------------------ 9.7/24.4 MB 512.6 kB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 519.1 kB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 525.9 kB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 525.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 533.0 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 539.2 kB/s eta 0:00:26\n",
      "   ------------------ --------------------- 11.0/24.4 MB 545.2 kB/s eta 0:00:25\n",
      "   ------------------ --------------------- 11.0/24.4 MB 545.2 kB/s eta 0:00:25\n",
      "   ------------------ --------------------- 11.3/24.4 MB 551.2 kB/s eta 0:00:24\n",
      "   ------------------ --------------------- 11.5/24.4 MB 557.7 kB/s eta 0:00:24\n",
      "   ------------------- -------------------- 11.8/24.4 MB 563.9 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 12.1/24.4 MB 569.4 kB/s eta 0:00:22\n",
      "   -------------------- ------------------- 12.3/24.4 MB 575.1 kB/s eta 0:00:21\n",
      "   -------------------- ------------------- 12.3/24.4 MB 575.1 kB/s eta 0:00:21\n",
      "   -------------------- ------------------- 12.6/24.4 MB 580.2 kB/s eta 0:00:21\n",
      "   --------------------- ------------------ 12.8/24.4 MB 585.5 kB/s eta 0:00:20\n",
      "   --------------------- ------------------ 13.1/24.4 MB 590.1 kB/s eta 0:00:20\n",
      "   --------------------- ------------------ 13.1/24.4 MB 590.1 kB/s eta 0:00:20\n",
      "   --------------------- ------------------ 13.4/24.4 MB 593.1 kB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 597.9 kB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 597.9 kB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 600.9 kB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 605.3 kB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 605.3 kB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 606.7 kB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 606.7 kB/s eta 0:00:17\n",
      "   ------------------------ --------------- 14.7/24.4 MB 608.3 kB/s eta 0:00:16\n",
      "   ------------------------ --------------- 14.7/24.4 MB 608.3 kB/s eta 0:00:16\n",
      "   ------------------------ --------------- 14.9/24.4 MB 608.9 kB/s eta 0:00:16\n",
      "   ------------------------ --------------- 15.2/24.4 MB 612.7 kB/s eta 0:00:16\n",
      "   ------------------------ --------------- 15.2/24.4 MB 612.7 kB/s eta 0:00:16\n",
      "   ------------------------- -------------- 15.5/24.4 MB 614.8 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 15.5/24.4 MB 614.8 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 15.7/24.4 MB 615.9 kB/s eta 0:00:15\n",
      "   -------------------------- ------------- 16.0/24.4 MB 618.0 kB/s eta 0:00:14\n",
      "   -------------------------- ------------- 16.0/24.4 MB 618.0 kB/s eta 0:00:14\n",
      "   -------------------------- ------------- 16.3/24.4 MB 618.4 kB/s eta 0:00:14\n",
      "   -------------------------- ------------- 16.3/24.4 MB 618.4 kB/s eta 0:00:14\n",
      "   --------------------------- ------------ 16.5/24.4 MB 620.2 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 16.5/24.4 MB 620.2 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 16.8/24.4 MB 619.8 kB/s eta 0:00:13\n",
      "   --------------------------- ------------ 17.0/24.4 MB 624.4 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 17.0/24.4 MB 624.4 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 17.0/24.4 MB 624.4 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 618.9 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 618.9 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 620.1 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 620.1 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 620.1 kB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 616.8 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 616.8 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 617.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 617.9 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 617.9 kB/s eta 0:00:11\n",
      "   ------------------------------ --------- 18.4/24.4 MB 615.4 kB/s eta 0:00:10\n",
      "   ------------------------------ --------- 18.6/24.4 MB 619.6 kB/s eta 0:00:10\n",
      "   ------------------------------ --------- 18.6/24.4 MB 619.6 kB/s eta 0:00:10\n",
      "   ------------------------------ --------- 18.6/24.4 MB 619.6 kB/s eta 0:00:10\n",
      "   ------------------------------ --------- 18.9/24.4 MB 619.9 kB/s eta 0:00:09\n",
      "   ------------------------------- -------- 19.1/24.4 MB 624.0 kB/s eta 0:00:09\n",
      "   ------------------------------- -------- 19.4/24.4 MB 632.1 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.4/24.4 MB 632.1 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.4/24.4 MB 632.1 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 19.7/24.4 MB 630.2 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 19.9/24.4 MB 637.0 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 19.9/24.4 MB 637.0 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 19.9/24.4 MB 637.0 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 19.9/24.4 MB 637.0 kB/s eta 0:00:08\n",
      "   --------------------------------- ------ 20.2/24.4 MB 635.0 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.2/24.4 MB 635.0 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.2/24.4 MB 635.0 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.2/24.4 MB 635.0 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.2/24.4 MB 635.0 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.4/24.4 MB 641.5 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.4/24.4 MB 641.5 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.4/24.4 MB 641.5 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.4/24.4 MB 641.5 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 20.7/24.4 MB 648.6 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 20.7/24.4 MB 648.6 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 20.7/24.4 MB 648.6 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 640.9 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 624.7 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 624.7 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 624.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 619.1 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 625.0 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 625.0 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 625.0 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 625.0 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 625.0 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 22.0/24.4 MB 620.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.0/24.4 MB 620.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.0/24.4 MB 620.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.3/24.4 MB 628.8 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.5/24.4 MB 617.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.5/24.4 MB 617.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.5/24.4 MB 617.7 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 22.8/24.4 MB 606.8 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 22.8/24.4 MB 606.8 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 22.8/24.4 MB 606.8 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.1/24.4 MB 600.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.1/24.4 MB 600.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.1/24.4 MB 600.1 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 23.3/24.4 MB 594.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.3/24.4 MB 594.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.6/24.4 MB 593.1 kB/s eta 0:00:02\n",
      "   ---------------------------------------  23.9/24.4 MB 548.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 548.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 548.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 548.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 548.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 548.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 517.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 517.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 509.3 kB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f73c419-0309-4794-b9e0-8786514744d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:15 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:16 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:25:16.058332', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Matrix...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m matrix_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m subset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 29\u001b[0m     vector \u001b[38;5;241m=\u001b[39m sentence_to_vector(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], model) \n\u001b[0;32m     30\u001b[0m     matrix_list\u001b[38;5;241m.\u001b[39mappend(vector)\n\u001b[0;32m     32\u001b[0m matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(matrix_list)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "wv_wrapper = WordVector() \n",
    "model = wv_wrapper.get_model() \n",
    "\n",
    "def sentence_to_vector(text, model):\n",
    "    words = word_tokenize(str(text), engine='newmm')\n",
    "    \n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index: \n",
    "            vecs.append(model[word])\n",
    "    \n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size) # Gensim ‡πÉ‡∏ä‡πâ .vector_size\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Matrix...\")\n",
    "subset = df.head(3) \n",
    "matrix_list = []\n",
    "\n",
    "for index, row in subset.iterrows():\n",
    "    vector = sentence_to_vector(row['text'], model) \n",
    "    matrix_list.append(vector)\n",
    "\n",
    "matrix = np.array(matrix_list)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: {matrix.shape}\")\n",
    "print(\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Matrix ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å):\")\n",
    "print(matrix[0][:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68970cf2-bf47-4a1c-a591-2aae586e3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:48 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "\n",
      "‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: ['\\t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏£‡∏π ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á \"\"‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á\"\" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏õ‡∏±‡∏à‡πÄ‡∏à‡∏Å‡∏ä‡∏ô‡∏™‡∏π‡∏á ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏à‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà \"\"‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ\"\"']\n",
      "** ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'body_text' ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '\t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏£‡∏π ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á \"\"‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á\"\" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏õ‡∏±‡∏à‡πÄ‡∏à‡∏Å‡∏ä‡∏ô‡∏™‡∏π‡∏á ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏à‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà \"\"‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ\"\"' ‡πÅ‡∏ó‡∏ô **\n",
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• thai2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:49 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:25:49.501792', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '\t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏£‡∏π ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á \"\"‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á\"\" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏õ‡∏±‡∏à‡πÄ‡∏à‡∏Å‡∏ä‡∏ô‡∏™‡∏π‡∏á ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏à‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà \"\"‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ\"\"' ‡πÄ‡∏õ‡πá‡∏ô Matrix...\n",
      "------------------------------\n",
      "Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: (10, 300)\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏£‡∏π ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á \"\"‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á\"\" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏õ‡∏±‡∏à‡πÄ‡∏à‡∏Å‡∏ä‡∏ô‡∏™‡∏π‡∏á ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏à‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà \"\"‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ\"\"):\n",
      "[ 0.01124822 -0.08241265  0.01800983 -0.05700911 -0.013285    0.04307939\n",
      " -0.11007562  0.06403525  0.03190831  0.04353578]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "\n",
    "# --- 1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå ---\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')\n",
    "    \n",
    "    # !!! ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ !!!\n",
    "    print(f\"\\n‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: {list(df.columns)}\") \n",
    "    \n",
    "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ (Prachatai ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ 'body_text' ‡∏´‡∏£‡∏∑‡∏≠ 'title')\n",
    "    # ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô 'body_text' ‡∏î‡∏π‡∏Ñ‡∏£‡∏±‡∏ö\n",
    "    target_col = 'body_text' \n",
    "    \n",
    "    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ 'body_text' ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ 'title' ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î\n",
    "    if target_col not in df.columns:\n",
    "        target_col = df.columns[0] # ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà print ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏´‡πá‡∏ô\n",
    "        print(f\"** ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'body_text' ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{target_col}' ‡πÅ‡∏ó‡∏ô **\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ---\n",
    "print(\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• thai2Vec...\")\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "def sentence_to_vector(text, model):\n",
    "    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠ ‡∏Å‡∏±‡∏ô Error ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "    words = word_tokenize(str(text), engine='newmm')\n",
    "    \n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "            \n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# --- 3. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
    "print(f\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{target_col}' ‡πÄ‡∏õ‡πá‡∏ô Matrix...\")\n",
    "subset = df.head(10) # ‡∏ï‡∏±‡∏î‡∏°‡∏≤ 10 ‡πÅ‡∏ñ‡∏ß\n",
    "matrix_list = []\n",
    "\n",
    "for index, row in subset.iterrows():\n",
    "    # ‡πÉ‡∏ä‡πâ target_col ‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ 'text'\n",
    "    vector = sentence_to_vector(row[target_col], model) \n",
    "    matrix_list.append(vector)\n",
    "\n",
    "matrix = np.array(matrix_list)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: {matrix.shape}\")\n",
    "print(f\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {target_col}):\")\n",
    "print(matrix[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed96a609-536c-47dd-9d6f-872fb707bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:42 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "** ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'body_text' ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '\t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏£‡∏π ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á \"\"‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á\"\" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏õ‡∏±‡∏à‡πÄ‡∏à‡∏Å‡∏ä‡∏ô‡∏™‡∏π‡∏á ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏à‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà \"\"‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ\"\"' ‡πÅ‡∏ó‡∏ô **\n",
      "‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 11 ‡πÅ‡∏ñ‡∏ß\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:43 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:25:43.524249', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1004eda037847998b084e71e473fe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: (11, 300)\n",
      "[ 0.01124822 -0.08241265  0.01800983 -0.05700911 -0.013285    0.04307939\n",
      " -0.11007562  0.06403525  0.03190831  0.04353578]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "from tqdm.auto import tqdm  # <--- ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ñ‡∏ö‡πÇ‡∏´‡∏•‡∏î\n",
    "\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')\n",
    "    \n",
    "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢\n",
    "    target_col = 'body_text' \n",
    "    if target_col not in df.columns:\n",
    "        target_col = df.columns[0]\n",
    "        print(f\"** ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'body_text' ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{target_col}' ‡πÅ‡∏ó‡∏ô **\")\n",
    "\n",
    "    print(f\"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "    exit()\n",
    "\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "def sentence_to_vector(text, model):\n",
    "    words = word_tokenize(str(text), engine='newmm')\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "            \n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "matrix_list = []\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ tqdm ‡∏Ñ‡∏£‡∏≠‡∏ö df.iterrows() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå‡πÅ‡∏ñ‡∏ö‡∏ß‡∏¥‡πà‡∏á\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    vector = sentence_to_vector(row[target_col], model) \n",
    "    matrix_list.append(vector)\n",
    "\n",
    "matrix = np.array(matrix_list)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: {matrix.shape}\") # ‡πÄ‡∏•‡∏Ç‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå\n",
    "print(f\"{matrix[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d55caa94-4973-46ec-9104-6ab78604b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:58 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 12 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:25:58 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:25:58.740662', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (Cleaning + Tokenizing + Vectorizing)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b8bc7ae58f4b8eb43a7ab835f5d3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! Matrix Size: (12, 300)\n",
      "------------------------------\n",
      "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1) ---\n",
      "üî¥ ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡πâ‡∏≤‡∏á:\n",
      "\t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ...\n",
      "\n",
      "üü¢ ‡∏´‡∏•‡∏±‡∏á‡∏•‡πâ‡∏≤‡∏á:\n",
      "‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤ ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤...\n",
      "\n",
      "‚úÇÔ∏è ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ:\n",
      "['‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ', ' ', '‡∏°‡∏µ', '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô', ' ', '(', '‡∏Ñ‡∏ô', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ', ' ', '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤', ' ', '‡πÉ‡∏ô', '‡∏™‡∏±‡∏á‡∏Ñ‡∏°', ')', ' ', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å', ' ', '‡∏ã‡∏∂‡πà‡∏á', '‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # <--- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Regular Expression ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Cleaning Function) ---\n",
    "def clean_text(text):\n",
    "    text = str(text) # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏Å‡πà‡∏≠‡∏ô\n",
    "    \n",
    "    # 1. ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà (\\n) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    # 2. ‡∏•‡∏ö‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠‡πÅ‡∏ö‡∏ö‡πÑ‡∏ó‡∏¢/‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å ‡πÄ‡∏ä‡πà‡∏ô ‡πë) 1. (‡πë) \n",
    "    # (‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏ß‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö)\n",
    "    text = re.sub(r'[\\(\\)\\[\\]‡πë-‡πô0-9]+\\)', '', text)\n",
    "    \n",
    "    # 3. ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô \"\" ‡∏´‡∏£‡∏∑‡∏≠ \" ‡∏≠‡∏≠‡∏Å\n",
    "    text = text.replace('\"', '').replace(\"'\", \"\")\n",
    "    \n",
    "    # 4. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡πÄ‡∏Ñ‡∏≤‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏£‡∏£‡∏Ñ‡∏¢‡∏≤‡∏ß‡πÜ) ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏≤‡∏∞‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# --- 2. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå ---\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "try:\n",
    "    # ‡πÉ‡∏ä‡πâ header=None ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip', header=None)\n",
    "    \n",
    "    # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏á‡πà‡∏≤‡∏¢‡πÜ (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å)\n",
    "    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏•‡∏Ç 0 ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç index ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 1, 2)\n",
    "    target_col = 0 \n",
    "    \n",
    "    print(f\"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ---\n",
    "print(\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\")\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "# --- 4. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ---\n",
    "print(f\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (Cleaning + Tokenizing + Vectorizing)...\")\n",
    "\n",
    "matrix_list = []\n",
    "tokens_list = [] \n",
    "cleaned_text_list = [] # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏ß‡πâ‡∏î‡∏π‡πÄ‡∏•‡πà‡∏ô\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # 4.1 ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö\n",
    "    raw_text = row[target_col]\n",
    "    \n",
    "    # 4.2 ** ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ **\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    cleaned_text_list.append(cleaned_text) # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á‡∏•‡∏¥‡∏™‡∏ï‡πå\n",
    "    \n",
    "    # 4.3 ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥\n",
    "    words = word_tokenize(cleaned_text, engine='newmm')\n",
    "    tokens_list.append(words)\n",
    "    \n",
    "    # 4.4 ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Vector\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "            \n",
    "    if len(vecs) == 0:\n",
    "        vector = np.zeros(model.vector_size)\n",
    "    else:\n",
    "        vector = np.mean(vecs, axis=0)\n",
    "        \n",
    "    matrix_list.append(vector)\n",
    "\n",
    "# --- 5. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ---\n",
    "matrix = np.array(matrix_list)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ô‡∏ä‡∏±‡∏î‡πÜ\n",
    "result_df = pd.DataFrame({\n",
    "    'Original': df[target_col],          # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°\n",
    "    'Cleaned': cleaned_text_list,        # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß (‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°)\n",
    "    'Tokens': tokens_list                # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ\n",
    "})\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! Matrix Size: {matrix.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á : ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏•‡πâ‡∏≤‡∏á vs ‡∏´‡∏•‡∏±‡∏á‡∏•‡πâ‡∏≤‡∏á\n",
    "print(\"--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1) ---\")\n",
    "print(f\"üî¥ ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡πâ‡∏≤‡∏á:\\n{result_df['Original'].iloc[0][:100]}...\")\n",
    "print(f\"\\nüü¢ ‡∏´‡∏•‡∏±‡∏á‡∏•‡πâ‡∏≤‡∏á:\\n{result_df['Cleaned'].iloc[0][:100]}...\")\n",
    "print(f\"\\n‚úÇÔ∏è ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ:\\n{result_df['Tokens'].iloc[0][:20]}\") # ‡πÇ‡∏ä‡∏ß‡πå 20 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ad0463-0efa-444f-a3d7-880db310bef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:26:02 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "** ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'body_text' ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '\t‡πë) ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô (‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ \"\"‡∏õ‡∏±‡∏ç‡∏ç‡∏≤\"\" ‡πÉ‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏£‡∏π ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á \"\"‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á\"\" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏õ‡∏±‡∏à‡πÄ‡∏à‡∏Å‡∏ä‡∏ô‡∏™‡∏π‡∏á ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏à‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà \"\"‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ\"\"' ‡πÅ‡∏ó‡∏ô **\n",
      "‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 11 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:26:03 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:26:03.652029', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defe1eb0d58645d3b0f7e8ab92e41249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\n",
      "Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: (11, 300)\n",
      "------------------------------\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ (‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å): ['\\t', '‡πí', ')', ' ', '‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç', '‡πÉ‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà', ' ', '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á', '‡∏Å‡∏±‡∏ö', '‡∏Å‡∏≤‡∏£', '‡∏Ç‡∏≤‡∏î', '‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°', '‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á', ' ', '‡∏´‡∏•‡∏ö‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á', '‡∏Å‡∏é', '‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤', '‡∏™‡∏±‡∏á‡∏Ñ‡∏°', ' ', '‡∏Ç‡∏≤‡∏î', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á', '‡∏ä‡∏≠‡∏ö‡∏ò‡∏£‡∏£‡∏°', ' ', '\\xa0\"\"', '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì', '\"\"', ' ', '‡∏´‡∏≤‡∏Å', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏û‡∏£‡∏∞', ' ', '‡∏Å‡πá', '‡∏Å‡∏≥‡∏•‡∏±‡∏á', '‡∏ñ‡∏π‡∏Å', '‡∏û‡∏£‡∏∞', '‡πÉ‡∏ô', '‡∏ß‡∏±‡∏î', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ', ' ', '‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà', ' ', '\"\"', '‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì', ' ', '‡∏õ', '‡∏£‡∏≤‡∏ä‡∏¥', '‡∏Å‡∏≠', '‡∏≠‡∏Å', '‡πÑ‡∏õ', '\"\"']\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Vector (‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å): [ 0.01124822 -0.08241265  0.01800983 -0.05700911 -0.013285    0.04307939\n",
      " -0.11007562  0.06403525  0.03190831  0.04353578]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize # <--- ‡πÇ‡∏´‡∏•‡∏î word_tokenize ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå ---\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')\n",
    "    \n",
    "    target_col = 'body_text' \n",
    "    if target_col not in df.columns:\n",
    "        target_col = df.columns[0]\n",
    "        print(f\"** ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 'body_text' ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{target_col}' ‡πÅ‡∏ó‡∏ô **\")\n",
    "\n",
    "    print(f\"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ---\n",
    "print(\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\")\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "# --- 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ + ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Vector) ---\n",
    "print(f\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...\")\n",
    "\n",
    "matrix_list = []\n",
    "tokens_list = [] # <--- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏°‡∏≤‡∏£‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    text = str(row[target_col])\n",
    "    \n",
    "    # 3.1 ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ (Word Tokenize)\n",
    "    words = word_tokenize(text, engine='newmm')\n",
    "    tokens_list.append(words) # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå\n",
    "    \n",
    "    # 3.2 ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Vector\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "            \n",
    "    if len(vecs) == 0:\n",
    "        vector = np.zeros(model.vector_size)\n",
    "    else:\n",
    "        vector = np.mean(vecs, axis=0)\n",
    "        \n",
    "    matrix_list.append(vector)\n",
    "\n",
    "# --- 4. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---\n",
    "matrix = np.array(matrix_list)\n",
    "\n",
    "# !!! ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏™‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ !!!\n",
    "df['tokenized_words'] = tokens_list \n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")\n",
    "print(f\"Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: {matrix.shape}\")\n",
    "print(\"-\" * 30)\n",
    "# ‡πÇ‡∏ä‡∏ß‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ (‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏™‡∏ß‡∏¢‡πÑ‡∏´‡∏°)\n",
    "print(f\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ (‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å): {df['tokenized_words'][0]}\") \n",
    "print(f\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Vector (‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å): {matrix[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cd891dd-e849-4174-82ca-8b8efc99367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥...\n",
      "--------------------------------------------------\n",
      "‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡∏ó‡∏±‡πâ‡∏á 12 ‡πÅ‡∏ñ‡∏ß!\n",
      "--------------------------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1: (‡∏°‡∏µ 64 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ', ' ', '‡∏°‡∏µ', '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏ä‡∏ô', ' ', '(', '‡∏Ñ‡∏ô', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ', ' ', '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤', ' ', '‡πÉ‡∏ô', '‡∏™‡∏±‡∏á‡∏Ñ‡∏°', ')'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 2: (‡∏°‡∏µ 45 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç', '‡πÉ‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÑ‡∏•‡πà', ' ', '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á', '‡∏Å‡∏±‡∏ö', '‡∏Å‡∏≤‡∏£', '‡∏Ç‡∏≤‡∏î', '‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°', '‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á', ' ', '‡∏´‡∏•‡∏ö‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á', '‡∏Å‡∏é', '‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤', '‡∏™‡∏±‡∏á‡∏Ñ‡∏°'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 3: (‡∏°‡∏µ 50 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì', ' ', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏ß‡πà‡∏≤', '‡∏à‡∏∞', '‡πÑ‡∏°‡πà', '‡∏≠‡∏≠‡∏Å', ' ', '(', '‡πÑ‡∏°‡πà', '‡∏™‡∏∂‡∏Å', ')', ' ', '‡πÅ‡∏ï‡πà', '‡∏à‡∏∞'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 4: (‡∏°‡∏µ 14 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏ó‡∏±‡∏Å‡∏©‡∏¥‡∏ì', ' ', '‡∏°‡∏µ', '‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°', '‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á', ' ', '‡πÅ‡∏ï‡πà', '‡∏à‡∏∞', '‡πÉ‡∏´‡πâ', '‡πÅ‡∏Å‡πâ', '‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç', ' ', '‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', '!'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 5: (‡∏°‡∏µ 32 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏Å‡∏≤‡∏£', '‡∏à‡∏∞', '‡∏ó‡∏≥', '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏°‡∏ï‡∏¥', '‡πÉ‡∏ô', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', ' ', '‡πë‡πô', ' ', '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô', ' ', '‡πí‡πï‡πî‡πô', ' ', '‡∏ß‡πà‡∏≤', '‡∏à‡∏∞'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 6: (‡∏°‡∏µ 16 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏Å‡∏≤‡∏£', '‡∏ó‡∏≥', '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏°‡∏ï‡∏¥', '‡∏à‡∏∞', '‡∏ï‡πâ‡∏≠‡∏á', '‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô', '‡∏ß‡πà‡∏≤', '‡∏à‡∏∞', '‡πÅ‡∏Å‡πâ', '‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç', '‡πÉ‡∏ô', '‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô', '‡πÉ‡∏î', ' ', '‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 7: (‡∏°‡∏µ 17 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏à‡∏∞', '‡∏ï‡πâ‡∏≠‡∏á', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏Å‡πà‡∏≠‡∏ô', '‡∏ß‡∏±‡∏ô', '‡∏•‡∏á‡∏°‡∏ï‡∏¥', '‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á', ' ', '‡πô‡πê', ' ', '‡∏ß‡∏±‡∏ô', ' ', '-', ' ', '‡πë‡πí‡πê'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 8: (‡∏°‡∏µ 28 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏à‡∏∞', '‡∏ï‡πâ‡∏≠‡∏á', '‡πÉ‡∏´‡πâ', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ó‡∏±‡πâ‡∏á', ' ', '‡πí', ' ', '‡∏î‡πâ‡∏≤‡∏ô‡πÉ‡∏ô', '‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô', '‡∏ó‡∏µ‡πà‡∏à‡∏∞', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', ' ', '‡πÇ‡∏î‡∏¢', '‡∏à‡∏∞'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 9: (‡∏°‡∏µ 10 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏°‡∏ï‡∏¥', '‡∏Ç‡∏≠‡∏á', '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô', '‡∏à‡∏∞', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£', ' ', '‡πÑ‡∏°‡πà', '‡∏ú‡∏π‡∏Å‡∏û‡∏±‡∏ô', '‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 10: (‡∏°‡∏µ 11 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡πÅ‡∏Ñ‡πà', '‡∏û‡∏π‡∏î', '‡∏™‡πà‡∏á', '‡πÜ', ' ', '‡πÇ‡∏î‡∏¢', '‡πÑ‡∏°‡πà', '‡∏£‡∏π‡πâ', '‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢', '‡πÅ‡∏•‡∏∞', '‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 11: (‡∏°‡∏µ 51 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡∏ô‡∏∂‡∏Å', '‡∏≠‡∏¢‡∏≤‡∏Å', '‡∏à‡∏∞', '‡∏ó‡∏≥', '‡∏≠‡∏∞‡πÑ‡∏£', '‡∏Å‡πá‡πÑ‡∏î‡πâ', ' ', '‡πÉ‡∏´‡πâ', '‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢', '‡∏ß‡∏¥‡πà‡∏á', '‡∏ï‡∏≤‡∏°', ' ', '‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£', '‡πÉ‡∏´‡πâ', '‡πÄ‡∏ô‡∏ï‡∏¥'] ...\n",
      "------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 12: (‡∏°‡∏µ 12 ‡∏Ñ‡∏≥)\n",
      "Tokenized: ['‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£', '‡∏ö‡∏¥‡∏î‡πÄ‡∏ö‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô', '‡∏Ç‡∏≠‡∏á', '‡∏™‡∏±‡∏á‡∏Ñ‡∏°', ' ', '‡∏Å‡∏•‡∏ö‡πÄ‡∏Å‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î', '‡∏Ç‡∏≠‡∏á', '‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á', ' ', '‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏ß‡∏•‡∏≤'] ...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pythainlp import word_tokenize\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Cleaning)\n",
    "# ---------------------------------------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    # ‡∏•‡∏ö‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠‡πÅ‡∏ö‡∏ö‡πÑ‡∏ó‡∏¢/‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å ‡πÅ‡∏•‡∏∞‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡πë) 1. (‡πë)\n",
    "    text = re.sub(r'[\\(\\)\\[\\]‡πë-‡πô0-9]+\\)', '', text)\n",
    "    # ‡∏•‡∏ö‡∏ü‡∏±‡∏ô‡∏´‡∏ô‡∏π‡∏ã‡πâ‡∏≥‡πÜ\n",
    "    text = text.replace('\"', '').replace(\"'\", \"\")\n",
    "    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ (Tokenize)\n",
    "# ---------------------------------------------------------\n",
    "def process_tokenize(text):\n",
    "    # ‡∏•‡πâ‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô\n",
    "    cleaned = clean_text(text)\n",
    "    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ engine 'newmm' (‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)\n",
    "    words = word_tokenize(cleaned, engine='newmm')\n",
    "    return words\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
    "# ---------------------------------------------------------\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥...\")\n",
    "\n",
    "try:\n",
    "    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip', header=None)\n",
    "    \n",
    "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (index 0)\n",
    "    target_col = 0\n",
    "    \n",
    "    # --- ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß ---\n",
    "    # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏≥‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "    df['tokens'] = df[target_col].apply(process_tokenize)\n",
    "    \n",
    "    # ‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏ß‡∏¢\n",
    "    df['word_count'] = df['tokens'].apply(len)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡∏ó‡∏±‡πâ‡∏á {len(df)} ‡πÅ‡∏ñ‡∏ß!\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {index+1}: (‡∏°‡∏µ {row['word_count']} ‡∏Ñ‡∏≥)\")\n",
    "        # ‡πÇ‡∏ä‡∏ß‡πå‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 15 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å‡∏û‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û)\n",
    "        print(f\"Tokenized: {row['tokens'][:15]} ...\") \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d881eb18-8165-45c8-993c-9f3ed8ba596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:26:14 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "   ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 12 ‡πÅ‡∏ñ‡∏ß\n",
      "2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:26:14 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:26:14.645621', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c0db0fe1fd42e4a3e2aeb637d66551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\n",
      "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix (Shape): (12, 300)\n",
      "----------------------------------------\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Vector ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1, 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å):\n",
      "[-0.03508672 -0.07794218  0.09421091  0.01403059 -0.01070807 -0.01077407\n",
      " -0.08723121  0.06308561  0.10746724  0.21166267]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ)\n",
    "# ==========================================\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    # ‡∏•‡∏ö‡∏û‡∏ß‡∏Å‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô ‡πë) 1. (‡πë)\n",
    "    text = re.sub(r'[\\(\\)\\[\\]‡πë-‡πô0-9]+\\)', '', text)\n",
    "    # ‡∏•‡∏ö‡∏ü‡∏±‡∏ô‡∏´‡∏ô‡∏π‡∏ã‡πâ‡∏≥‡πÜ \"\" ‡∏´‡∏£‡∏∑‡∏≠ \"\n",
    "    text = text.replace('\"', '').replace(\"'\", \"\")\n",
    "    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô Vector (Thai2Vec)\n",
    "def sentence_to_vector(text, model):\n",
    "    # 1. ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥\n",
    "    cleaned = clean_text(text)\n",
    "    # 2. ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥\n",
    "    words = word_tokenize(cleaned, engine='newmm')\n",
    "    \n",
    "    # 3. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    \n",
    "    # 4. ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 0)\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# ==========================================\n",
    "# 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å\n",
    "# ==========================================\n",
    "\n",
    "# --- A. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå ---\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "\n",
    "try:\n",
    "    # header=None ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô)\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip', header=None)\n",
    "    \n",
    "    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å (index 0)\n",
    "    target_col = 0 \n",
    "    \n",
    "    # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 11 ‡πÅ‡∏ñ‡∏ß (‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ)\n",
    "    # df = df.head(11) # <--- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏∑‡πà‡∏ô‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏ó‡∏™‡πÅ‡∏Ñ‡πà 11 ‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ\n",
    "    \n",
    "    print(f\"   ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- B. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ---\n",
    "print(\"2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\")\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "# --- C. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á Matrix ---\n",
    "print(\"3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Matrix...\")\n",
    "matrix_list = []\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ tqdm ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÅ‡∏ñ‡∏ö‡πÇ‡∏´‡∏•‡∏î\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    vector = sentence_to_vector(row[target_col], model)\n",
    "    matrix_list.append(vector)\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á List ‡πÄ‡∏õ‡πá‡∏ô Numpy Array (Matrix ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)\n",
    "final_matrix = np.array(matrix_list)\n",
    "\n",
    "# ==========================================\n",
    "# 3. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "# ==========================================\n",
    "print(\"-\" * 40)\n",
    "print(f\"‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
    "print(f\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix (Shape): {final_matrix.shape}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Vector ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1, 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å):\")\n",
    "print(final_matrix[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b663319-c757-4e19-bcaf-d3f2f15e533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:26:22 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "   ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 12 ‡πÅ‡∏ñ‡∏ß\n",
      "2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:26:23 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:26:23.436421', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Matrix ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea9a5b1b0cc4107bbfc2fd4107fd03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "‡πÑ‡∏î‡πâ Matrix ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 12 ‡∏≠‡∏±‡∏ô\n",
      "--------------------------------------------------\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (44, 300) (‡∏°‡∏µ 44 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 2: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (34, 300) (‡∏°‡∏µ 34 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 3: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (39, 300) (‡∏°‡∏µ 39 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 4: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (11, 300) (‡∏°‡∏µ 11 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 5: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (23, 300) (‡∏°‡∏µ 23 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 6: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (14, 300) (‡∏°‡∏µ 14 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 7: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (12, 300) (‡∏°‡∏µ 12 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 8: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (25, 300) (‡∏°‡∏µ 25 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 9: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (9, 300) (‡∏°‡∏µ 9 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 10: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (10, 300) (‡∏°‡∏µ 10 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 11: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (38, 300) (‡∏°‡∏µ 38 ‡∏Ñ‡∏≥)\n",
      "‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 12: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = (10, 300) (‡∏°‡∏µ 10 ‡∏Ñ‡∏≥)\n",
      "--------------------------------------------------\n",
      "Matrix:\n",
      "[[-0.685299 -0.223269  0.087079 ...  0.078487 -0.027761 -0.278003]\n",
      " [-0.156962 -0.231863  0.080312 ...  0.031696 -0.670379 -0.008048]\n",
      " [-0.010273 -0.023921  0.084533 ... -0.038695 -0.037515 -0.020538]\n",
      " [-0.120522 -0.355783  0.16818  ... -0.382658 -1.205359  0.340139]\n",
      " [-0.606954  0.48889   0.065101 ...  0.663205 -0.472645 -0.287729]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î\n",
    "# ==========================================\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    text = re.sub(r'[\\(\\)\\[\\]‡πë-‡πô0-9]+\\)', '', text)\n",
    "    text = text.replace('\"', '').replace(\"'\", \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================================\n",
    "# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Matrix (‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡∏°‡πà: ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢)\n",
    "# ==========================================\n",
    "def text_to_matrix(text, model):\n",
    "    # 1. ‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥\n",
    "    cleaned = clean_text(text)\n",
    "    words = word_tokenize(cleaned, engine='newmm')\n",
    "    \n",
    "    # 2. ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô Vector\n",
    "    vecs = []\n",
    "    found_words = [] # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡πÑ‡∏ß‡πâ‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô map ‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "            found_words.append(word)\n",
    "            \n",
    "    # 3. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô map ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Matrix ‡∏ß‡πà‡∏≤‡∏á‡πÜ\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros((0, model.vector_size)), []\n",
    "    \n",
    "    # 4. *** ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô Matrix (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥ x 300) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á mean ***\n",
    "    return np.array(vecs), found_words\n",
    "\n",
    "# ==========================================\n",
    "# 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
    "# ==========================================\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\dataset_10.csv'\n",
    "print(\"1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip', header=None)\n",
    "    target_col = 0 \n",
    "    print(f\"   ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "print(\"2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\")\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "# --- ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô List of Matrices ---\n",
    "print(\"3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Matrix ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß...\")\n",
    "list_of_matrices = [] # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö Matrix 12 ‡∏≠‡∏±‡∏ô\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß\n",
    "    matrix, words = text_to_matrix(row[target_col], model)\n",
    "    \n",
    "    # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á List\n",
    "    list_of_matrices.append(matrix)\n",
    "\n",
    "# ==========================================\n",
    "# 4. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "# ==========================================\n",
    "print(\"-\" * 50)\n",
    "print(f\"‡πÑ‡∏î‡πâ Matrix ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(list_of_matrices)} ‡∏≠‡∏±‡∏ô\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ‡∏•‡∏≠‡∏á‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏π‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Matrix\n",
    "for i, mat in enumerate(list_of_matrices):\n",
    "    # mat.shape ‡∏à‡∏∞‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥, 300)\n",
    "    print(f\"‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {i+1}: ‡∏Ç‡∏ô‡∏≤‡∏î Matrix = {mat.shape} (‡∏°‡∏µ {mat.shape[0]} ‡∏Ñ‡∏≥)\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Matrix:\")\n",
    "if len(list_of_matrices) > 0:\n",
    "    print(list_of_matrices[0][:5]) # ‡∏î‡∏π 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44eb20ca-80bc-4769-a6f7-5ef97e0e7b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
      "1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test/Validation...\n",
      "   - Train set: 7 ‡∏Ç‡πà‡∏≤‡∏ß\n",
      "   - Validation set: 2 ‡∏Ç‡πà‡∏≤‡∏ß\n",
      "   - Test set: 3 ‡∏Ç‡πà‡∏≤‡∏ß\n",
      "2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:609: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(), mlp)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏ó‡∏£‡∏ô (Fit) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 4. ‡∏ß‡∏±‡∏î‡∏ú‡∏• (Evaluation)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Evaluation)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[1;32m--> 662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:754\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    738\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, incremental\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:476\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# Run the Stochastic optimization solver\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;129;01min\u001b[39;00m _STOCHASTIC_SOLVERS:\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stochastic(\n\u001b[0;32m    477\u001b[0m         X,\n\u001b[0;32m    478\u001b[0m         y,\n\u001b[0;32m    479\u001b[0m         activations,\n\u001b[0;32m    480\u001b[0m         deltas,\n\u001b[0;32m    481\u001b[0m         coef_grads,\n\u001b[0;32m    482\u001b[0m         intercept_grads,\n\u001b[0;32m    483\u001b[0m         layer_units,\n\u001b[0;32m    484\u001b[0m         incremental,\n\u001b[0;32m    485\u001b[0m     )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:660\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_stochastic\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss = \u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_))\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# update no_improvement_count based on training loss or\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m# validation score according to early_stopping\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_no_improvement_count(early_stopping, X_val, y_val)\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# for learning rate that needs to be updated at iteration end\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39miteration_ends(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:708\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._update_no_improvement_count\u001b[1;34m(self, early_stopping, X_val, y_val)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_no_improvement_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, early_stopping, X_val, y_val):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stopping:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;66;03m# compute validation score (can be NaN), use that for stopping\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m         val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score(X_val, y_val)\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_scores_\u001b[38;5;241m.\u001b[39mappend(val_score)\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1194\u001b[0m, in \u001b[0;36mMLPClassifier._score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_score_with_function(X, y, score_function\u001b[38;5;241m=\u001b[39maccuracy_score)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:769\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._score_with_function\u001b[1;34m(self, X, y, score_function)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;66;03m# Input validation would remove feature names, so we disable it\u001b[39;00m\n\u001b[0;32m    767\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(y_pred)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(y_pred)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score_function(y, y_pred)\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ==========================================\n",
    "# 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)\n",
    "# ==========================================\n",
    "\n",
    "# X ‡∏Ñ‡∏∑‡∏≠ Matrix ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏à‡∏≤‡∏Å Thai2Vec (‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)\n",
    "X = final_matrix  \n",
    "\n",
    "# y ‡∏Ñ‡∏∑‡∏≠ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö/‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß \n",
    "# !!! ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô 'category' ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏∏‡∏ì !!!\n",
    "# ‡πÄ‡∏ä‡πà‡∏ô df[1], df['tags'], df['category']\n",
    "if len(df.columns) > 1:\n",
    "    y = df[1] # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà 2 (index 1) ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "else:\n",
    "    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á ‡∏ú‡∏°‡∏à‡∏∞‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏Ñ‡∏£‡∏±‡∏ö\n",
    "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö\")\n",
    "    y = ['‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á'] * len(X) # (‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ô‡∏∞ ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏Ñ‡πà‡∏Å‡∏±‡∏ô Error)\n",
    "\n",
    "# ==========================================\n",
    "# 2. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Train / Validation / Test)\n",
    "# ==========================================\n",
    "print(\"1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test/Validation...\")\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1: ‡πÅ‡∏¢‡∏Å Test ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ 20% (‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 80% ‡πÑ‡∏ß‡πâ Train+Val)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2: ‡∏à‡∏≤‡∏Å 80% ‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏¢‡∏Å Validation ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏µ‡∏Å 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   - Train set: {X_train.shape[0]} ‡∏Ç‡πà‡∏≤‡∏ß\")\n",
    "print(f\"   - Validation set: {X_val.shape[0]} ‡∏Ç‡πà‡∏≤‡∏ß\")\n",
    "print(f\"   - Test set: {X_test.shape[0]} ‡∏Ç‡πà‡∏≤‡∏ß\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡πÅ‡∏•‡∏∞ MLP Classifier (‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ã‡∏µ‡∏ô)\n",
    "# ==========================================\n",
    "print(\"2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô...\")\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ MLP ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ã‡∏µ‡∏ô‡∏ó‡∏≥\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),  # <--- 2 Hidden Layers (‡∏ä‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å 128, ‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏≠‡∏á 64)\n",
    "    batch_size=64,                 # <--- Mini-batch size = 64\n",
    "    activation='relu',             # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "    solver='adam',                 # ‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "    max_iter=500,                  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏Å‡∏±‡∏ô‡∏°‡∏±‡∏ô‡∏£‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö)\n",
    "    random_state=42,\n",
    "    early_stopping=True,           # ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤)\n",
    "    validation_fraction=0.1        # ‡πÅ‡∏ö‡πà‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏ä‡πá‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n",
    ")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline: ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• -> ‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "pipeline = make_pipeline(StandardScaler(), mlp)\n",
    "\n",
    "# ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏ó‡∏£‡∏ô (Fit) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 4. ‡∏ß‡∏±‡∏î‡∏ú‡∏• (Evaluation)\n",
    "# ==========================================\n",
    "print(\"3. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Evaluation)...\")\n",
    "\n",
    "# ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö (Predict) ‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î Test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"üìä Classification Report:\")\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ Precision, Recall, F1-Score ‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ F1 Score ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏ß‡∏° (‡πÅ‡∏ö‡∏ö Weighted ‡∏Ñ‡∏∑‡∏≠‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"‚≠ê Weighted F1 Score: {f1:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "515efc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ: ['culture' 'economics' 'education' 'environment' 'human_rights' 'ict'\n",
      " 'international' 'labor' 'national_security' 'other' 'politics'\n",
      " 'quality_of_life' 'social']\n",
      "\n",
      "4. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
      "   Train: 35302 | Val: 8826 | Test: 11032\n",
      "\n",
      "5. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•...\n",
      "\n",
      "6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•...\n",
      "--------------------------------------------------\n",
      "üìä Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          culture       0.31      0.46      0.37       114\n",
      "        economics       0.43      0.29      0.35       151\n",
      "        education       0.33      0.41      0.37        41\n",
      "      environment       0.64      0.47      0.54       684\n",
      "     human_rights       0.54      0.41      0.47      1135\n",
      "              ict       0.33      0.06      0.11        31\n",
      "    international       0.49      0.51      0.50       576\n",
      "            labor       0.22      0.04      0.07        49\n",
      "national_security       0.38      0.08      0.13       100\n",
      "            other       0.51      0.55      0.53       594\n",
      "         politics       0.77      0.84      0.81      6365\n",
      "  quality_of_life       0.52      0.55      0.54       986\n",
      "           social       0.24      0.06      0.09       206\n",
      "\n",
      "         accuracy                           0.68     11032\n",
      "        macro avg       0.44      0.37      0.37     11032\n",
      "     weighted avg       0.66      0.68      0.66     11032\n",
      "\n",
      "‚≠ê Weighted F1 Score: 0.6624\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° X ‡πÅ‡∏•‡∏∞ y (‡πÄ‡∏û‡∏¥‡πà‡∏° LabelEncoder)\n",
    "# ==========================================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = final_matrix\n",
    "y_raw = df['category']\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á category ‡∏à‡∏≤‡∏Å string ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "print(\"‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ:\", label_encoder.classes_)\n",
    "\n",
    "# ==========================================\n",
    "# 5. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Val/Test\n",
    "# ==========================================\n",
    "print(\"\\n4. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"   Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏• MLP\n",
    "# ==========================================\n",
    "print(\"\\n5. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    batch_size=64,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), mlp)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 7. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
    "# ==========================================\n",
    "print(\"\\n6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•...\")\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"üìä Classification Report:\")\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"‚≠ê Weighted F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9231300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\n",
      "   ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: 55160 ‡πÅ‡∏ñ‡∏ß\n",
      "   ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: ['Unnamed: 0', 'url', 'date', 'title', 'body_text', 'politics', 'human_rights', 'quality_of_life', 'international', 'social', 'environment', 'economics', 'culture', 'labor', 'national_security', 'ict', 'education']\n",
      "   ‡∏û‡∏ö‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: ['politics', 'human_rights', 'quality_of_life', 'international', 'social', 'environment', 'economics', 'culture', 'labor', 'national_security', 'ict', 'education']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:27:10 - INFO - \t loading projection weights from C:\\Users\\hasbo\\pythainlp-data\\thai2vec.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'category' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\n",
      "category\n",
      "politics             31398\n",
      "human_rights          5671\n",
      "quality_of_life       4904\n",
      "environment           3597\n",
      "other                 3209\n",
      "international         2900\n",
      "social                 971\n",
      "economics              781\n",
      "culture                613\n",
      "national_security      493\n",
      "labor                  225\n",
      "ict                    209\n",
      "education              189\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2026 15:27:11 - INFO - \t KeyedVectors lifecycle event {'msg': 'loaded (51358, 300) matrix of type float32 from C:\\\\Users\\\\hasbo\\\\pythainlp-data\\\\thai2vec.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-01-06T15:27:11.209594', 'gensim': '4.4.0', 'python': '3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á 'body_text' ‡πÄ‡∏õ‡πá‡∏ô Matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92372a9e674497bbc8453409e00ecc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: (55160, 300)\n",
      "\n",
      "4. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
      "   Train: 35302 | Val: 8826 | Test: 11032\n",
      "\n",
      "5. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 128\u001b[0m\n\u001b[0;32m    116\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPClassifier(\n\u001b[0;32m    117\u001b[0m     hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m    118\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     validation_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(), mlp)\n\u001b[1;32m--> 128\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# 7. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[1;32m--> 662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:754\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    738\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, incremental\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:476\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# Run the Stochastic optimization solver\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;129;01min\u001b[39;00m _STOCHASTIC_SOLVERS:\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stochastic(\n\u001b[0;32m    477\u001b[0m         X,\n\u001b[0;32m    478\u001b[0m         y,\n\u001b[0;32m    479\u001b[0m         activations,\n\u001b[0;32m    480\u001b[0m         deltas,\n\u001b[0;32m    481\u001b[0m         coef_grads,\n\u001b[0;32m    482\u001b[0m         intercept_grads,\n\u001b[0;32m    483\u001b[0m         layer_units,\n\u001b[0;32m    484\u001b[0m         incremental,\n\u001b[0;32m    485\u001b[0m     )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:660\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_stochastic\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss = \u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_))\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# update no_improvement_count based on training loss or\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m# validation score according to early_stopping\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_no_improvement_count(early_stopping, X_val, y_val)\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# for learning rate that needs to be updated at iteration end\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39miteration_ends(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:708\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._update_no_improvement_count\u001b[1;34m(self, early_stopping, X_val, y_val)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_no_improvement_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, early_stopping, X_val, y_val):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stopping:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;66;03m# compute validation score (can be NaN), use that for stopping\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m         val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score(X_val, y_val)\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_scores_\u001b[38;5;241m.\u001b[39mappend(val_score)\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1194\u001b[0m, in \u001b[0;36mMLPClassifier._score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_score_with_function(X, y, score_function\u001b[38;5;241m=\u001b[39maccuracy_score)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:769\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._score_with_function\u001b[1;34m(self, X, y, score_function)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;66;03m# Input validation would remove feature names, so we disable it\u001b[39;00m\n\u001b[0;32m    767\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(y_pred)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(y_pred)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score_function(y, y_pred)\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0. Imports ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pythainlp.word_vector import WordVector\n",
    "from pythainlp import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    text = re.sub(r'[\\(\\)\\[\\]‡πë-‡πô0-9]+\\)', '', text)\n",
    "    text = text.replace('\"', '').replace(\"'\", \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô Vector\n",
    "def sentence_to_vector(text, model):\n",
    "    cleaned = clean_text(text)\n",
    "    words = word_tokenize(cleaned, engine='newmm')\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# ==========================================\n",
    "# 1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Prachatai (‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà)\n",
    "# ==========================================\n",
    "file_path = r'D:\\New folder\\‡∏á‡∏≤‡∏ô‡∏°.‡∏®‡∏¥‡∏•\\New folder\\pythoch\\prachatai_train.csv'\n",
    "print(\"1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå...\")\n",
    "df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')\n",
    "print(f\"   ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "print(f\"   ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {list(df.columns)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Label (y) ‡∏à‡∏≤‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
    "# ==========================================\n",
    "# ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà ‡πÄ‡∏ä‡πà‡∏ô politics, human_rights ‡∏Ø‡∏•‡∏Ø (‡πÄ‡∏õ‡πá‡∏ô 0/1)\n",
    "# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏•‡∏±‡∏Å‡πÜ ‡πÄ‡∏ä‡πà‡∏ô 'politics' ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "\n",
    "# ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏≥ Multi-class: ‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏°‡∏ß‡∏î‡πÑ‡∏´‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "category_cols = ['politics', 'human_rights', 'quality_of_life', 'international', \n",
    "                 'social', 'environment', 'economics', 'culture', \n",
    "                 'labor', 'national_security', 'ict', 'education']\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°\n",
    "available_cats = [c for c in category_cols if c in df.columns]\n",
    "print(f\"   ‡∏û‡∏ö‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: {available_cats}\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á y ‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ 1 (‡∏´‡∏£‡∏∑‡∏≠ \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\" ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏°‡∏î)\n",
    "def get_category(row):\n",
    "    for cat in available_cats:\n",
    "        if row[cat] == 1:\n",
    "            return cat\n",
    "    return 'other'  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏•‡∏¢\n",
    "\n",
    "df['category'] = df.apply(get_category, axis=1)\n",
    "print(f\"   ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'category' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# ==========================================\n",
    "# 3. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Matrix\n",
    "# ==========================================\n",
    "print(\"\\n2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Vec...\")\n",
    "wv_wrapper = WordVector()\n",
    "model = wv_wrapper.get_model()\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'body_text' ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß\n",
    "target_col = 'body_text'\n",
    "\n",
    "print(f\"3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á '{target_col}' ‡πÄ‡∏õ‡πá‡∏ô Matrix...\")\n",
    "matrix_list = []\n",
    "\n",
    "# ‚ö†Ô∏è ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏û‡∏±‡∏Å\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    vector = sentence_to_vector(row[target_col], model)\n",
    "    matrix_list.append(vector)\n",
    "\n",
    "final_matrix = np.array(matrix_list)\n",
    "print(f\"   Matrix ‡∏Ç‡∏ô‡∏≤‡∏î: {final_matrix.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° X ‡πÅ‡∏•‡∏∞ y\n",
    "# ==========================================\n",
    "X = final_matrix\n",
    "y = df['category']\n",
    "\n",
    "# ==========================================\n",
    "# 5. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Val/Test\n",
    "# ==========================================\n",
    "print(\"\\n4. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"   Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏• MLP\n",
    "# ==========================================\n",
    "print(\"\\n5. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    batch_size=64,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), mlp)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 7. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
    "# ==========================================\n",
    "print(\"\\n6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•...\")\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"‚≠ê Weighted F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fad512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
