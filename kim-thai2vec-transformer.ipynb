{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Thai Text Classification with Transformer\n",
    "### Encoder: thai2vec | Decoder: Transformer\n",
    "---\n",
    "**Settings:**\n",
    "- MAX_LEN = 256\n",
    "- BATCH_SIZE = 128\n",
    "- EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T03:02:37.692093Z",
     "iopub.status.busy": "2026-01-09T03:02:37.691771Z",
     "iopub.status.idle": "2026-01-09T03:02:43.848892Z",
     "shell.execute_reply": "2026-01-09T03:02:43.848033Z",
     "shell.execute_reply.started": "2026-01-09T03:02:37.692066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Install dependencies\n",
    "!pip install -q transformers scikit-learn pythainlp pandas gensim x-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T03:02:43.851075Z",
     "iopub.status.busy": "2026-01-09T03:02:43.850722Z",
     "iopub.status.idle": "2026-01-09T03:03:09.897900Z",
     "shell.execute_reply": "2026-01-09T03:03:09.897148Z",
     "shell.execute_reply.started": "2026-01-09T03:02:43.851035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ“š Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp import word_vector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸŸ¢ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# âš™ï¸ Settings\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "CHECKPOINT_EVERY = 10  # à¸šà¸±à¸™à¸—à¸¶à¸ checkpoint à¸—à¸¸à¸ 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ“‚ Load Data\n",
    "# âš ï¸ Upload à¹„à¸Ÿà¸¥à¹Œ prachatai_train.csv à¹„à¸›à¸—à¸µà¹ˆ Kaggle Input à¸à¹ˆà¸­à¸™!\n",
    "# à¹à¸à¹‰ path à¸•à¸²à¸¡à¸—à¸µà¹ˆ upload\n",
    "\n",
    "# à¸ªà¸³à¸«à¸£à¸±à¸š Kaggle:\n",
    "# df = pd.read_csv(\"/kaggle/input/your-dataset/prachatai_train.csv\")\n",
    "\n",
    "# à¸ªà¸³à¸«à¸£à¸±à¸š Colab (upload à¹„à¸Ÿà¸¥à¹Œà¸à¹ˆà¸­à¸™):\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv(\"prachatai_train.csv\")\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/prachatai-train/prachatai_train.csv\")\n",
    "\n",
    "texts = df[\"body_text\"].astype(str).tolist()\n",
    "label_cols = [\n",
    "    \"politics\", \"human_rights\", \"quality_of_life\", \"international\",\n",
    "    \"social\", \"environment\", \"economics\", \"culture\", \"labor\",\n",
    "    \"national_security\", \"ict\", \"education\"\n",
    "]\n",
    "y = df[label_cols].values.astype(np.float32)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset size: {len(texts)}\")\n",
    "print(f\"ğŸ“‹ Labels: {label_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ”¤ Load thai2vec Word Embedding\n",
    "print(\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸” thai2fit_wv...\")\n",
    "model_wv = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "w2v = model_wv\n",
    "embedding_dim = w2v.vector_size\n",
    "\n",
    "print(f\"âœ… Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"à¸à¸³à¸¥à¸±à¸‡ tokenize...\")\n",
    "tokenized_texts = [word_tokenize(t, keep_whitespace=False) for t in texts]\n",
    "\n",
    "# Build vocabulary\n",
    "w2v_vocab = list(w2v.key_to_index.keys())\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, word in enumerate(w2v_vocab, start=2):\n",
    "    vocab[word] = i\n",
    "\n",
    "print(f\"âœ… Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ Encode & Pad\n",
    "def encode_text(tokens, vocab):\n",
    "    return [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
    "\n",
    "def pad_sequences(sequences, max_len=256, pad_value=0):\n",
    "    padded = np.full((len(sequences), max_len), pad_value, dtype=np.int64)\n",
    "    lengths = np.array([min(len(seq), max_len) for seq in sequences], dtype=np.int64)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :min(len(seq), max_len)] = seq[:max_len]\n",
    "    return padded, lengths\n",
    "\n",
    "encoded_texts = [encode_text(tokens, vocab) for tokens in tokenized_texts]\n",
    "X, lengths = pad_sequences(encoded_texts, max_len=MAX_LEN)\n",
    "\n",
    "print(f\"âœ… X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š Train/Test Split\n",
    "X_train, X_test, y_train, y_test, len_train, len_test = train_test_split(\n",
    "    X, y, lengths, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train size: {len(X_train)}\")\n",
    "print(f\"âœ… Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ“¦ Dataset & DataLoader\n",
    "class ThaiTextDataset(Dataset):\n",
    "    def __init__(self, X, lengths, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.lengths[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ThaiTextDataset(X_train, len_train, y_train)\n",
    "test_dataset = ThaiTextDataset(X_test, len_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, \n",
    "                         num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"âœ… Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ§  Build Embedding Matrix\n",
    "vocab_size = max(vocab.values()) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in w2v:\n",
    "        embedding_matrix[idx] = w2v[word]\n",
    "    elif word == \"<PAD>\":\n",
    "        embedding_matrix[idx] = np.zeros(embedding_dim)\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "print(f\"âœ… Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ¤– Transformer Model\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, output_dim, \n",
    "                 depth=4, heads=4, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = TransformerWrapper(\n",
    "            num_tokens=vocab_size,\n",
    "            max_seq_len=max_seq_len,\n",
    "            attn_layers=Decoder(  # ğŸ‘ˆ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ Encoder à¹€à¸›à¹‡à¸™ Decoder!\n",
    "                dim=embed_dim,\n",
    "                depth=depth,\n",
    "                heads=heads,\n",
    "                attn_dropout=0.1,\n",
    "                ff_dropout=0.1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if embedding_matrix is not None:\n",
    "            self.transformer.token_emb.emb.weight.data.copy_(\n",
    "                torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(embed_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, text, lengths=None):\n",
    "        transformer_out = self.transformer(text, return_embeddings=True)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            mask = torch.arange(text.size(1), device=text.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "            mask = mask.unsqueeze(-1).float()\n",
    "            pooled = (transformer_out * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            pooled = transformer_out.mean(dim=1)\n",
    "        \n",
    "        out = self.fc1(pooled)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ Create Model\n",
    "output_dim = len(label_cols)\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=embedding_dim,\n",
    "    max_seq_len=MAX_LEN,\n",
    "    output_dim=output_dim,\n",
    "    depth=2,\n",
    "    heads=6,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "print(f\"âœ… Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ Training with Mixed Precision\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = GradScaler()  # Mixed Precision\n",
    "\n",
    "best_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# ğŸ“‚ Resume from checkpoint if exists\n",
    "checkpoint_path = 'checkpoint.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"ğŸ“‚ Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    print(f\"âœ… Resumed from epoch {start_epoch}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Starting training from epoch {start_epoch + 1}...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (X_batch, lengths_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Forward\n",
    "        with autocast():\n",
    "            outputs = model(X_batch, lengths_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Mixed Precision Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Progress bar\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % CHECKPOINT_EVERY == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"ğŸ’¾ Checkpoint saved at epoch {epoch + 1}\")\n",
    "    \n",
    "    # Estimate remaining time\n",
    "    elapsed = time.time() - total_start_time\n",
    "    eta = (elapsed / (epoch - start_epoch + 1)) * (EPOCHS - epoch - 1) / 60\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {epoch_time:.1f}s | ETA: {eta:.1f}min\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"ğŸ“Š Best loss: {best_loss:.4f}\")\n",
    "print(f\"â±ï¸ Total time: {total_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T03:03:33.615742Z",
     "iopub.status.busy": "2026-01-09T03:03:33.615426Z",
     "iopub.status.idle": "2026-01-09T03:03:33.623405Z",
     "shell.execute_reply": "2026-01-09T03:03:33.622510Z",
     "shell.execute_reply.started": "2026-01-09T03:03:33.615715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1764566775.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_55/1764566775.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    model.load_state_dict(torch.load(/kaggle/input/number1/pytorch/default/1/best_transformer_model.pth))\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ˆ Evaluation\n",
    "model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch, y_batch in test_loader:\n",
    "        X_batch, lengths_batch = X_batch.to(device), lengths_batch.to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(X_batch, lengths_batch)\n",
    "        \n",
    "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        y_true.append(y_batch.numpy())\n",
    "        y_pred.append(preds)\n",
    "\n",
    "y_true = np.vstack(y_true)\n",
    "y_pred = np.vstack(y_pred)\n",
    "\n",
    "print(f\"ğŸ“Š F1-score (macro): {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"ğŸ“Š F1-score (micro): {f1_score(y_true, y_pred, average='micro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T03:02:27.497605Z",
     "iopub.status.busy": "2026-01-09T03:02:27.496947Z",
     "iopub.status.idle": "2026-01-09T03:02:27.509771Z",
     "shell.execute_reply": "2026-01-09T03:02:27.508777Z",
     "shell.execute_reply.started": "2026-01-09T03:02:27.497563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/3446104168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"à¸£à¸±à¸à¸šà¸²à¸¥à¹„à¸—à¸¢à¸›à¸£à¸°à¸à¸²à¸¨à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸”à¹‰à¸²à¸™à¸ªà¸´à¹ˆà¸‡à¹à¸§à¸”à¸¥à¹‰à¸­à¸¡à¹ƒà¸«à¸¡à¹ˆ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"à¹à¸£à¸‡à¸‡à¸²à¸™à¸›à¸£à¸°à¸—à¹‰à¸§à¸‡à¹€à¸à¸·à¹ˆà¸­à¸ªà¸´à¸—à¸˜à¸´à¹ƒà¸™à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/3446104168.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ğŸ”® Prediction Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ğŸ”® Prediction Function\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(text, keep_whitespace=False)\n",
    "    ids = encode_text(tokens, vocab)[:MAX_LEN]\n",
    "    \n",
    "    # Pad\n",
    "    padded_ids = ids + [0] * (MAX_LEN - len(ids))\n",
    "    \n",
    "    lengths = torch.tensor([len(ids)], dtype=torch.long).to(device)\n",
    "    padded = torch.tensor([padded_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            output = model(padded, lengths)\n",
    "        probs = torch.sigmoid(output).cpu().numpy()[0]\n",
    "        \n",
    "        # Multi-label results\n",
    "        results = [(label_cols[i], float(probs[i])) \n",
    "                   for i in range(len(probs)) if probs[i] > 0.5]\n",
    "        \n",
    "        if not results:\n",
    "            best_idx = np.argmax(probs)\n",
    "            results = [(label_cols[best_idx], float(probs[best_idx]))]\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test\n",
    "print(predict(\"à¸£à¸±à¸à¸šà¸²à¸¥à¹„à¸—à¸¢à¸›à¸£à¸°à¸à¸²à¸¨à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸”à¹‰à¸²à¸™à¸ªà¸´à¹ˆà¸‡à¹à¸§à¸”à¸¥à¹‰à¸­à¸¡à¹ƒà¸«à¸¡à¹ˆ\"))\n",
    "print(predict(\"à¹à¸£à¸‡à¸‡à¸²à¸™à¸›à¸£à¸°à¸—à¹‰à¸§à¸‡à¹€à¸à¸·à¹ˆà¸­à¸ªà¸´à¸—à¸˜à¸´à¹ƒà¸™à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ’¾ Download model (for Colab)\n",
    "# from google.colab import files\n",
    "# files.download('best_transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9221986,
     "sourceId": 14437544,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 555658,
     "modelInstanceId": 542458,
     "sourceId": 713858,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
