import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import os
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW


# ==========================================
# 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Setup)
# ==========================================
print("--- Step 1: Setup ---")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

MODEL_NAME = "airesearch/wangchanberta-base-att-spm-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
CACHE_FILE = "cached_data_bert.pt" # ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß

file_name = r"d:\year4\‡∏™‡∏´‡∏Å‡∏¥‡∏à\prachatai_test.csv"
df = pd.read_csv(file_name)

label_cols = ['politics', 'human_rights', 'quality_of_life', 'international', 
              'social', 'environment', 'economics', 'culture', 'labor', 
              'national_security', 'ict', 'education']

# ==========================================
# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)
# ==========================================
def prepare_data(df, tokenizer, max_len=256):
    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå Cache -> ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏•‡∏¢ ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å
    if os.path.exists(CACHE_FILE):
        print(f"‚úÖ ‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå Cache '{CACHE_FILE}'... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏™‡∏π‡∏ï‡∏£‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏∞!)")
        saved_data = torch.load(CACHE_FILE)
        
        train_dataset = TensorDataset(saved_data['train_inputs'], saved_data['train_masks'], saved_data['train_labels'])
        test_dataset = TensorDataset(saved_data['test_inputs'], saved_data['test_masks'], saved_data['test_labels'])
        pos_weights_tensor = saved_data['pos_weights'].to(device)
        
        print("-> ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        return train_dataset, test_dataset, pos_weights_tensor

    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå -> ‡∏ó‡∏≥‡πÉ‡∏´‡∏°‡πà (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÉ‡∏´‡∏°‡πà)
    print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå Cache... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà")
    
    texts = df['body_text'].values
    labels = df[label_cols].values 

    # --- [‡πÅ‡∏Å‡πâ‡∏™‡∏π‡∏ï‡∏£‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÉ‡∏ä‡πâ Sqrt ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏£‡∏á] ---
    num_samples = len(df)
    counts = df[label_cols].sum().values
    
    # ‡∏™‡∏π‡∏ï‡∏£‡πÄ‡∏î‡∏¥‡∏°: (Total - Count) / Count  <-- ‡πÅ‡∏£‡∏á‡πÑ‡∏õ
    # ‡∏™‡∏π‡∏ï‡∏£‡πÉ‡∏´‡∏°‡πà: Sqrt( (Total - Count) / Count ) <-- ‡∏ô‡∏∏‡πà‡∏°‡∏ô‡∏ß‡∏•‡∏Ç‡∏∂‡πâ‡∏ô
    raw_weights = (num_samples - counts) / np.maximum(counts, 1) # ‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£ 0
    pos_weights = np.sqrt(raw_weights) 
    
    pos_weights_tensor = torch.tensor(pos_weights, dtype=torch.float).to(device)
    
    print("\n--- Calculated Class Weights (Sqrt Dampened) ---")
    for i, col in enumerate(label_cols):
        print(f"  - {col}: {pos_weights[i]:.2f}")

    # ‡πÅ‡∏ö‡πà‡∏á Train/Test
    train_texts, test_texts, train_y, test_y = train_test_split(texts, labels, test_size=0.2, random_state=42)

    # Batch Tokenization
    def batch_encode(text_list):
        return tokenizer.batch_encode_plus(
            list(text_list),
            add_special_tokens=True,
            max_length=max_len,
            padding='max_length',
            truncation=True,
            return_token_type_ids=False,
            return_attention_mask=True,
            return_tensors='pt'
        )

    print("   -> Tokenizing Train Data...")
    train_enc = batch_encode(train_texts)
    print("   -> Tokenizing Test Data...")
    test_enc = batch_encode(test_texts)

    # ‡πÅ‡∏õ‡∏•‡∏á Label ‡πÄ‡∏õ‡πá‡∏ô Tensor
    train_labels = torch.tensor(train_y, dtype=torch.float)
    test_labels = torch.tensor(test_y, dtype=torch.float)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Cache
    print(f"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Cache ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå '{CACHE_FILE}'...")
    torch.save({
        'train_inputs': train_enc['input_ids'],
        'train_masks': train_enc['attention_mask'],
        'train_labels': train_labels,
        'test_inputs': test_enc['input_ids'],
        'test_masks': test_enc['attention_mask'],
        'test_labels': test_labels,
        'pos_weights': pos_weights_tensor
    }, CACHE_FILE)
    
    train_dataset = TensorDataset(train_enc['input_ids'], train_enc['attention_mask'], train_labels)
    test_dataset = TensorDataset(test_enc['input_ids'], test_enc['attention_mask'], test_labels)
    
    return train_dataset, test_dataset, pos_weights_tensor

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
train_dataset, test_dataset, pos_weights_tensor = prepare_data(df, tokenizer)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader
BATCH_SIZE = 16 
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

# ==========================================
# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model & Training)
# ==========================================
print(f"\n--- Step 2: Loading BERT Model ---")
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=len(label_cols),
    problem_type="multi_label_classification"
)
model = model.to(device)

optimizer = AdamW(model.parameters(), lr=3e-5)

# Loss Function ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights_tensor)

print("\n--- Step 3: Start Fine-tuning ---")
EPOCHS = 5 # ‡∏£‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠!

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    
    for batch in train_loader:
        # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô TensorDataset: 0=input_ids, 1=mask, 2=labels
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(outputs.logits, labels)
        
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}")

# ==========================================
# 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Save Model)
# ==========================================
print("\n--- Saving Model ---")
output_dir = "./my_bert_multilabel_model"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏µ‡πà: {output_dir}")

# ==========================================
# 5. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (Prediction)
# ==========================================
print("\n--- Step 4: Testing & Prediction ---")

def predict_news(text):
    model.eval()
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=256,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        probs = torch.sigmoid(outputs.logits) 
        
    probs = probs.cpu().detach().numpy()[0]
    
    print("-" * 50)
    print(f"[‡∏Ç‡πà‡∏≤‡∏ß]: {text[:80]}...")
    print(">> ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:")
    
    found = False
    # Threshold 0.5
    for i, prob in enumerate(probs):
        if prob > 0.5: 
            print(f"   ‚úÖ {label_cols[i]}: {prob*100:.2f}%")
            found = True
            
    if not found:
        print("   (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á 50% ‡πÅ‡∏ï‡πà‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠):")
        top_indices = probs.argsort()[-3:][::-1]
        for idx in top_indices:
            print(f"      - {label_cols[idx]}: {probs[idx]*100:.2f}%")

# ‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö
news_list = [
    "‡πÅ‡∏Æ‡∏Ñ‡πÄ‡∏Å‡∏≠‡∏£‡πå‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ‡∏Ç‡πÇ‡∏°‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏õ‡∏Ç‡∏≤‡∏¢‡∏ï‡πà‡∏≠", 
    "‡∏ä‡∏≤‡∏ß‡∏ö‡πâ‡∏≤‡∏ô‡∏ä‡∏∏‡∏°‡∏ô‡∏∏‡∏°‡∏Ñ‡∏±‡∏î‡∏Ñ‡πâ‡∏≤‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏á‡πÅ‡∏£‡πà ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏£‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°", 
    "‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏£‡πâ‡∏≠‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ ‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏£‡∏±‡∏ö‡∏õ‡∏≤‡∏Å‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤", 
    "‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå‡πÑ‡∏ó‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏ß‡∏≤‡∏î‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏ô‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•‡∏´‡∏ô‡∏±‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏Ñ‡∏≤‡∏ô‡∏™‡πå" 
]

for news in news_list:
    predict_news(news)